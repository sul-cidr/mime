{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import imageio.v3 as iio\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from matplotlib import colormaps, colors\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import umap\n",
    "\n",
    "from mime_db import MimeDb\n",
    "#from pose_functions import *\n",
    "\n",
    "MAX_PIXEL_MOVEMENT = 500 # Disregard movelets with > this many pixels movement (probably necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = \"JuliusCaesar-WinterMainStage23.mp4\" # Just the name of the video file, no path\n",
    "\n",
    "video_path = Path(\"videos\", VIDEO_FILE)\n",
    "\n",
    "# Connect to the database\n",
    "db = await MimeDb.create()\n",
    "\n",
    "# Get video metadata\n",
    "video_name = video_path.name\n",
    "print(video_name)\n",
    "video_id = await db.get_video_id(video_name)\n",
    "\n",
    "print(\"VIDEO ID\", video_id)\n",
    "\n",
    "video_data = await db.get_video_by_id(video_id)\n",
    "video_fps = video_data[\"fps\"]\n",
    "\n",
    "video_movelets = await db.get_movelet_data_from_video(video_id)\n",
    "movelets_df = pd.DataFrame.from_records(video_movelets, columns=video_movelets[0].keys())\n",
    "\n",
    "video_poses = await db.get_pose_data_from_video(video_id)\n",
    "#video_poses = await db.get_poses_with_faces(video_id)\n",
    "\n",
    "# frame_pose_data = await db.get_pose_data_by_frame(video_id)\n",
    "# frame_pose_df = pd.DataFrame.from_records(frame_pose_data, columns=frame_pose_data[0].keys())\n",
    "\n",
    "video_shots = await db.get_video_shots(video_id)\n",
    "shots_df = pd.DataFrame.from_records(video_shots, columns=video_shots[0].keys())\n",
    "\n",
    "poses_df = pd.DataFrame.from_records(video_poses, columns=video_poses[0].keys())\n",
    "\n",
    "print(poses_df.memory_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movelets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOTAL MOVELETS:\", len(movelets_df))\n",
    "print(\"NON-MOTION MOVELETS:\", len(movelets_df[movelets_df['movement'].isna()]))\n",
    "print(\"MOVELETS WITH STILL MOTION:\", len(movelets_df[movelets_df['movement'] == 0]))\n",
    "print(\"MOVELETS WITH MOVEMENT < 10px/sec:\", len(movelets_df[(movelets_df['movement'] >= 0) & (movelets_df['movement'] < 10)]))\n",
    "\n",
    "print(\"MEAN MOVEMENT PER MOVELET (norm px/sec):\", np.nanmean(movelets_df['movement']))\n",
    "print(\"MEDIAN MOVEMENT PER MOVELET (norm px/sec):\", np.nanmedian(movelets_df['movement']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnull_movelets_df = movelets_df.copy()\n",
    "#nonnull_movelets_df['movement'].fillna(-1, inplace=True)\n",
    "nonnull_movelets_df['movement'] = nonnull_movelets_df['movement'].fillna(-1)\n",
    "\n",
    "movement_distribution = nonnull_movelets_df[nonnull_movelets_df['movement'] <= MAX_PIXEL_MOVEMENT]['movement']\n",
    "\n",
    "n, bins, patches = plt.hist(movement_distribution, bins=300)\n",
    "plt.xlabel(\"Movement (normalized pixels/sec)\")\n",
    "plt.ylabel(\"# Movelets\")\n",
    "top_bin = n[1:].argmax()\n",
    "print('most frequent bin: (' + str(bins[top_bin]) + ',' + str(bins[top_bin+1]) + ')')\n",
    "print('mode: '+ str((bins[top_bin] + bins[top_bin+1])/2))\n",
    "movement_mode = (bins[top_bin] + bins[top_bin+1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe use frozen_poses to compare pose distributions? Or just use all poses in the show?\n",
    "frozen_movelets = movelets_df[(movelets_df['movement'] >= 0) & (movelets_df['movement'] < movement_mode)].reset_index()\n",
    "frozen_poses = frozen_movelets['norm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_df = poses_df.merge(shots_df, left_on=\"frame\", right_on=\"frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import skew, skewtest, kurtosis, kurtosistest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_distribution_stats(distrib, plot=False):\n",
    "\n",
    "    if skewtest(distrib).pvalue < .05:\n",
    "        skewness = skew(distrib)\n",
    "    else:\n",
    "        skewness = 0\n",
    "\n",
    "    if kurtosistest(distrib).pvalue < .05:\n",
    "        kurtosis_value = kurtosis(distrib)\n",
    "    else:\n",
    "        kurtosis_value = 0\n",
    "\n",
    "    if plot:\n",
    "        plt.hist(distrib, bins=\"auto\")  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "\n",
    "    return {\"count\": len(distrib), \"mean\": np.mean(distrib), \"median\": np.median(distrib), \"stdev\": np.std(distrib), \"skewness\": skewness, \"kurtosis\": kurtosis_value}\n",
    "\n",
    "\n",
    "def get_body_units(keypoints3d, camera):\n",
    "    return get_body_unit(project_pose(keypoints3d, camera))\n",
    "\n",
    "def get_body_unit(pose_proj):\n",
    "    shoulders = euclidean(pose_proj[1], pose_proj[2])\n",
    "    left_upper_arm = euclidean(pose_proj[1], pose_proj[3])\n",
    "    right_upper_arm = euclidean(pose_proj[2], pose_proj[4])\n",
    "    left_forearm = euclidean(pose_proj[3], pose_proj[5])\n",
    "    right_forearm = euclidean(pose_proj[4], pose_proj[6])\n",
    "    hips = euclidean(pose_proj[7], pose_proj[8])\n",
    "    left_thigh = euclidean(pose_proj[7], pose_proj[9])\n",
    "    right_thigh = euclidean(pose_proj[8], pose_proj[10])\n",
    "    left_lower_leg = euclidean(pose_proj[9], pose_proj[11])\n",
    "    right_lower_leg = euclidean(pose_proj[10], pose_proj[12])\n",
    "\n",
    "    print(shoulders, left_upper_arm, right_upper_arm, left_forearm, right_forearm, hips, left_thigh, right_thigh, left_lower_leg, right_lower_leg)\n",
    "\n",
    "    return np.mean([shoulders, left_upper_arm, right_upper_arm, left_forearm, right_forearm, hips, left_thigh, right_thigh, left_lower_leg, right_lower_leg])\n",
    "\n",
    "\n",
    "def project_pose(keypoints3d, camera):\n",
    "    kp_array = np.array(np.split(keypoints3d, 13))\n",
    "    kp_camera = np.array(camera)\n",
    "\n",
    "    return kp_array + kp_camera.T\n",
    "\n",
    "\n",
    "def get_projected_pose_centroid(keypoints3d, camera):\n",
    "\n",
    "    #proj_centroid = np.mean(kp_array, axis=0) + kp_camera.T\n",
    "    proj_centroid = np.mean(project_pose(keypoints3d, camera), axis=0)\n",
    "\n",
    "    proj_centroid[2] = proj_centroid[2]/200\n",
    "\n",
    "    return proj_centroid\n",
    "\n",
    "def get_projected_pose_bbox(keypoints3d, camera):\n",
    "    ppose = project_pose(keypoints3d, camera)\n",
    "    ppose[:,2] /= 200\n",
    "    mins = np.min(ppose, axis=0).tolist()\n",
    "    maxes = np.max(ppose, axis=0).tolist()\n",
    "    \n",
    "    return [mins, maxes]\n",
    "\n",
    "def are_overlapping(bbox1, bbox2):\n",
    "    # Each bbox is [[xmin, ymin, zmin], [xmax, ymax, zmax]]\n",
    "\n",
    "    return ((bbox1[1][0] >= bbox2[0][0] and bbox1[1][0] <= bbox2[1][0]) \\\n",
    "            or (bbox1[0][0] <= bbox2[1][0] and bbox1[0][0] >= bbox2[0][0])) \\\n",
    "            and ((bbox1[1][1] >= bbox2[0][1] and bbox1[1][1] <= bbox2[1][1]) \\\n",
    "            or (bbox1[0][1] <= bbox2[1][1] and bbox1[0][1] >= bbox2[0][1])) \\\n",
    "            and ((bbox1[1][2] >= bbox2[0][2] and bbox1[1][2] <= bbox2[1][2]) \\\n",
    "            or (bbox1[0][2] <= bbox2[1][2] and bbox1[0][2] >= bbox2[0][2]))\n",
    "\n",
    "    # From https://blender.stackexchange.com/questions/253355/collision-detection\n",
    "    # isColliding = ((x_max >= x_min2 and x_max <= x_max2) \\\n",
    "    #                 or (x_min <= x_max2 and x_min >= x_min2)) \\\n",
    "    #                 and ((y_max >= y_min2 and y_max <= y_max2) \\\n",
    "    #                 or (y_min <= y_max2 and y_min >= y_min2)) \\\n",
    "    #                 and ((z_max >= z_min2 and z_max <= z_max2) \\\n",
    "    #                 or (z_min <= z_max2 and z_min >= z_min2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_by_frame = poses_df.groupby(\"frame\")\n",
    "\n",
    "all_distances = []\n",
    "all_mean_distances_per_frame = []\n",
    "poses_per_populated_frame = []\n",
    "overlaps_in_frame = []\n",
    "\n",
    "for frame, frame_poses_df in poses_by_frame:\n",
    "\n",
    "    #print(\"FRAME\", frame, \"SHOT\", frame_poses_df.iloc[0][\"shot\"], \"POSES\", len(frame_poses_df))\n",
    "    \n",
    "    poses_per_populated_frame.append(len(frame_poses_df))\n",
    "\n",
    "    if len(frame_poses_df) <= 1:\n",
    "        continue\n",
    "\n",
    "    pose_centroids = frame_poses_df.apply(lambda x: get_projected_pose_centroid(x.keypoints3d, x.camera), axis=1)\n",
    "    #print(\"POSE CENTROIDS\")\n",
    "    pose_centroids = [list(centroid) for centroid in pose_centroids]\n",
    "    #print(pose_centroids)\n",
    "\n",
    "    dist_matrix = distance_matrix(pose_centroids, pose_centroids)\n",
    "\n",
    "    #print(\"DISTANCE MATRIX\")\n",
    "    #print(dist_matrix)\n",
    "\n",
    "    upper_diagonal = list(dist_matrix[np.triu_indices(len(dist_matrix), k=1)])\n",
    "\n",
    "    #print(\"UPPER DIAGONAL\")\n",
    "    #print(upper_diagonal)\n",
    "\n",
    "    mean_interpose_distance = np.mean(upper_diagonal)\n",
    "\n",
    "    all_distances.extend(upper_diagonal)\n",
    "    all_mean_distances_per_frame.append(mean_interpose_distance)\n",
    "\n",
    "    #if len(pose_centroids) > 1:\n",
    "    #    print(\"EUCLIDEAN\")\n",
    "    #    print(euclidean(pose_centroids[0], pose_centroids[1]))\n",
    "\n",
    "    pose_bboxes = frame_poses_df.apply(lambda x: get_projected_pose_bbox(x.keypoints3d, x.camera), axis=1)\n",
    "    pose_bboxes = [list(bbox) for bbox in pose_bboxes]\n",
    "\n",
    "    #print(\"POSE BOUNDING BOXES\")\n",
    "    #print(pose_bboxes)\n",
    "\n",
    "    total_overlaps = 0\n",
    "    for i, bbox1 in enumerate(pose_bboxes):\n",
    "        for j, bbox2 in enumerate(pose_bboxes):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            if are_overlapping(bbox1, bbox2):\n",
    "                print(\"OVERLAP DETECTED IN FRAME\", frame_poses_df.iloc[0][\"frame\"], \"SHOT\", frame_poses_df.iloc[0][\"shot\"], bbox1, bbox2)\n",
    "                total_overlaps += 1\n",
    "\n",
    "    if total_overlaps > 1:\n",
    "        overlaps_in_frame.append(total_overlaps)\n",
    "\n",
    "    #pose_body_units = shot_poses_df.apply(lambda x: get_body_units(x.keypoints3d, x.camera), axis=1)\n",
    "    #print(\"POSE BODY UNITS\")\n",
    "    #print(pose_body_units)\n",
    "\n",
    "    #print(shot_poses_df[\"camera\"]) # camera pos is relative to *each* detection\n",
    "    #if shot > 101:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"POSES PER POPULATED FRAME:\")\n",
    "print(get_distribution_stats(poses_per_populated_frame, True))\n",
    "\n",
    "print(\"OVERLAPS (# POSES OVERLAPPING):\")\n",
    "print(get_distribution_stats(overlaps_in_frame, True))\n",
    "\n",
    "print(\"MEAN INTERPOSE DISTANCES PER POPULATED FRAME:\")\n",
    "print(get_distribution_stats(all_mean_distances_per_frame, True))\n",
    "\n",
    "print(\"ALL INTERPOSE DISTANCES\")\n",
    "print(get_distribution_stats(all_distances, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge non-null movelets with poses to calculate sidereal movement per pose per frame|second\n",
    "\n",
    "def get_sidereal_motion(movelet):\n",
    "\n",
    "    # There shouldn't ever be more than one pose centroid that matches these queries (right???)\n",
    "    start_centroid = poses_df[(poses_df[\"track_id\"] == movelet[\"track_id\"]) & (poses_df[\"frame\"] == movelet[\"start_frame\"])][\"centroid_3d\"].iloc[0]\n",
    "    end_centroid = poses_df[(poses_df[\"track_id\"] == movelet[\"track_id\"]) & (poses_df[\"frame\"] == movelet[\"end_frame\"])][\"centroid_3d\"].iloc[0]\n",
    "    \n",
    "    return euclidean(start_centroid, end_centroid)\n",
    "\n",
    "#movelets_df = nonnull_movelets_df[nonnull_movelets_df['movement'] <= MAX_PIXEL_MOVEMENT]\n",
    "\n",
    "#poses_df[\"centroid_3d\"] = poses_df.apply(lambda x: get_projected_pose_centroid(x.keypoints3d, x.camera), axis=1)\n",
    "\n",
    "movelets_df[\"sidereal\"] = movelets_df.apply(get_sidereal_motion, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Need another statistic (replacing/in addition): sidereal motion per second, based on movelet duration\n",
    "\n",
    "print(\"SIDEREAL MOTION PER MOVELET\")\n",
    "print(get_distribution_stats(movelets_df[movelets_df[\"sidereal\"] < 1][\"sidereal\"].tolist(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_by_shot = poses_df.groupby(\"shot\")\n",
    "\n",
    "#all_distances = []\n",
    "\n",
    "tracks_per_shot = []\n",
    "\n",
    "for shot, shot_poses_df in poses_by_shot:\n",
    "\n",
    "    # if shot < 100:\n",
    "    #    continue\n",
    "    # if shot > 100:\n",
    "    #     break\n",
    "\n",
    "    total_tracks = shot_poses_df.track_id.unique()\n",
    "\n",
    "    if len(total_tracks):\n",
    "        print(shot, total_tracks)\n",
    "\n",
    "        tracks_per_shot.append(len(total_tracks))\n",
    "\n",
    "    continue\n",
    "\n",
    "    shot_poses_frames_df = shot_poses_df.groupby(\"frame\")\n",
    "\n",
    "    for frame, frame_poses_df in shot_poses_frames_df:\n",
    "\n",
    "        #print(\"FRAME\", frame, \"SHOT\", shot)\n",
    "\n",
    "        pose_centroids = frame_poses_df.apply(lambda x: get_projected_pose_centroid(x.keypoints3d, x.camera), axis=1)\n",
    "        #print(\"POSE CENTROIDS\")\n",
    "        pose_centroids = [list(centroid) for centroid in pose_centroids]\n",
    "        #print(pose_centroids)\n",
    "\n",
    "        dist_matrix = distance_matrix(pose_centroids, pose_centroids)\n",
    "\n",
    "        #print(\"DISTANCE MATRIX\")\n",
    "        #print(dist_matrix)\n",
    "\n",
    "        upper_diagonal = list(dist_matrix[np.triu_indices(len(dist_matrix), k=1)])\n",
    "\n",
    "        #print(\"UPPER DIAGONAL\")\n",
    "        #print(upper_diagonal)\n",
    "\n",
    "        all_distances.extend(upper_diagonal)\n",
    "\n",
    "        #if len(pose_centroids) > 1:\n",
    "        #    print(\"EUCLIDEAN\")\n",
    "        #    print(euclidean(pose_centroids[0], pose_centroids[1]))\n",
    "\n",
    "        pose_bboxes = frame_poses_df.apply(lambda x: get_projected_pose_bbox(x.keypoints3d, x.camera), axis=1)\n",
    "        pose_bboxes = [list(bbox) for bbox in pose_bboxes]\n",
    "\n",
    "        #print(\"POSE BOUNDING BOXES\")\n",
    "        #print(pose_bboxes)\n",
    "\n",
    "        total_overlaps = 0\n",
    "        for i, bbox1 in enumerate(pose_bboxes):\n",
    "            for j, bbox2 in enumerate(pose_bboxes):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                if are_overlapping(bbox1, bbox2):\n",
    "                    print(\"OVERLAP DETECTED IN FRAME\", frame_poses_df.iloc[0][\"frame\"], \"SHOT\", shot, shot_poses_df.iloc[0][\"shot\"], bbox1, bbox2)\n",
    "                    total_overlaps += 1\n",
    "\n",
    "        if total_overlaps > 1:\n",
    "            print(shot, \"TOTAL OVERLAPPING POSES:\", total_overlaps)\n",
    "\n",
    "        #pose_body_units = shot_poses_df.apply(lambda x: get_body_units(x.keypoints3d, x.camera), axis=1)\n",
    "        #print(\"POSE BODY UNITS\")\n",
    "        #print(pose_body_units)\n",
    "\n",
    "        #print(shot_poses_df[\"camera\"]) # camera pos is relative to *each* detection\n",
    "        #if shot > 101:\n",
    "        #    break\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRACKS PER SHOT:\")\n",
    "print(get_distribution_stats(tracks_per_shot, True))\n",
    "\n",
    "print(\"NORMALIZED MOVEMENT STATS:\")\n",
    "print(get_distribution_stats(movement_distribution, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Z distances seem to be exaggerated -- apparently by 200:1? See PHALP phalp/utils/utils.py\n",
    "- Focal length is 5000 (pixels?) -- not sure how to use this\n",
    "\n",
    "Based on body part ratios (i.e., considering average of all shoulder widths, waist widths, upper arm lengths and lower arm lengths) the projected 3D pose coordinate units appear to be in meters.\n",
    "\n",
    "\n",
    "## Metrics of interest for director-to-video clustering analysis\n",
    "\n",
    "* Difference of action recognition embedding vectors. How best to compare? Just cosine distance of averaged embeddings? Some kind of multi-dimensional KL divergence? Would be nice to be able to take into account the normalized histogram of values in each of the 62 elements of the action recognition vectors.\n",
    "\n",
    "* Difference of \"held\" 3D pose vectors/representations. How to compare? Maybe just 3D Euclidean distance between averaged normalized poses? As flattened embeddings, they'd be 100 x 100 x 100 = 1,000,000-element vectors (curse of dimensionality)\n",
    "\n",
    "## Motion:\n",
    "\n",
    "* Normalized: per-pixel movement within a pose's frame of reference, for each movelet -- already calculated\n",
    "* Sidereal (relative to the \"scenery\"): normalized per-meter (?) movement for each movelet -- need to join movelets with poses (again)\n",
    "\n",
    "## Closeness of figures in frames/shots:\n",
    "\n",
    "One primary shortcoming of this is that we usually can't see the whole stage, and reconstructing the entire scene based on multiple camera views (if those are provided at all) is beyond current technology, though various NeRF/Gaussian Splatting methods are getting closer. So we're mostly just stuck with what we get in each shot, which makes this analysis highly \"contaminated\" by the cinematography/cinematic style. Only way to sidestep this for now is to note that we're including multiple films for each stage director, so hopefully the influence of the filmic cinematography of any given recording will be attenuated.\n",
    "\n",
    "* For each populated frame in the film, with the denominator = # of frames with actual people detected in them. Ignore unpopulated frames; incorporate the frame rate to get all of these metrics per second:\n",
    "    * Get the dist of number of people per populated frame/sec\n",
    "* For the rest of these, only consider frames with multiple people:\n",
    "    * Get the distance dist between people per populated frame/sec (see below for calculations)\n",
    "    * Get the number of overlapping people per populated frame/sec (see below for calculations)\n",
    "\n",
    "### Secondary analyses\n",
    "\n",
    "* Maybe try to count and distribution-ize the number of distinct people in a shot -- this is essentially the number of tracks in a shot (tracking is done by appearance, including costumes though without much weight given to faces). Could help to cover the case of people coming into and out of a shot. Though it may just further confuse the issue, and in any case this should be very sensitive to shot length -- so may need to normalize by shot length somehow for the distribution to be useful.\n",
    "\n",
    "\n",
    "### Alternative shot-based analysis (probably not necessary)\n",
    "* For each shot:\n",
    "    * For the first (?) frame in the shot:\n",
    "        * For each pose in the frame:\n",
    "            * Get its position as the median of x, y, z/200 (?) coords\n",
    "            * Also get its 3D bounding box if doing collision detection\n",
    "\n",
    "        * Compute Euclidean distance matrix of all of the pose centroids, put this into a distribution, extract statistics\n",
    "            * Determine how many (if any) 3D bounding boxes intersect (touching) []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(np.array(np.split(poses_df.iloc[1000].keypoints3d, 13)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_df.iloc[1000].camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps = np.array_split(poses_df.iloc[0].keypoints4dh, 45)\n",
    "kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_movelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_embedding = umap.UMAP(\n",
    "    random_state=42,\n",
    ").fit_transform(frozen_poses)\n",
    "\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterable_embedding = umap.UMAP(\n",
    "    n_neighbors=10,\n",
    "    min_dist=1.0,\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    ").fit_transform(frozen_poses)\n",
    "\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting clustering model\")\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=3, min_samples=4) # , max_cluster_size=15\n",
    "hdb.fit(frozen_poses)\n",
    "labels = hdb.labels_.tolist()\n",
    "\n",
    "assigned_poses = 0\n",
    "\n",
    "cluster_to_poses = {}\n",
    "for i, cluster_id in enumerate(labels):\n",
    "    if cluster_id not in cluster_to_poses:\n",
    "        cluster_to_poses[cluster_id] = [i]\n",
    "    else:\n",
    "        cluster_to_poses[cluster_id].append(i)\n",
    "    \n",
    "tracks_per_cluster = []\n",
    "poses_per_track_per_cluster = []\n",
    "        \n",
    "for cluster_id in range(-1, max(labels) + 1):\n",
    "    #print(\"Poses in cluster\", cluster_id, labels.count(cluster_id))\n",
    "\n",
    "    cluster_track_poses = {}\n",
    "    for movelet_id in cluster_to_poses[cluster_id]:\n",
    "        movelet_track = frozen_movelets.iloc[movelet_id]['track_id']\n",
    "        if movelet_track not in cluster_track_poses:\n",
    "            cluster_track_poses[movelet_track] = 1\n",
    "        else:\n",
    "            cluster_track_poses[movelet_track] += 1\n",
    "            \n",
    "    if cluster_id != -1:\n",
    "        assigned_poses += labels.count(cluster_id)\n",
    "        tracks_per_cluster.append(len(cluster_track_poses))\n",
    "        poses_per_track_per_cluster.append(labels.count(cluster_id) / len(cluster_track_poses))\n",
    "    \n",
    "    #print(\"Tracks in cluster\", cluster_id, len(cluster_track_poses))\n",
    "\n",
    "print(\"assigned\", assigned_poses, \"poses out of\", len(labels), round(assigned_poses/len(labels),4))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,4))\n",
    "ax = fig2.gca()\n",
    "n, bins, patches = plt.hist(tracks_per_cluster, bins=20)\n",
    "ax.set_title(\"Tracks per cluster\")\n",
    "ax.set_xlabel(\"# Tracks\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()\n",
    "\n",
    "fig3 = plt.figure(figsize=(10,4))\n",
    "ax = fig3.gca()\n",
    "n, bins, patches = plt.hist(poses_per_track_per_cluster, bins=30)\n",
    "ax.set_title(\"Poses per track per cluster\")\n",
    "ax.set_xlabel(\"Poses/track\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting UMAP preclustered model\")\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=3, min_samples=4) # , max_cluster_size=15\n",
    "hdb.fit(clusterable_embedding)\n",
    "labels = hdb.labels_.tolist()\n",
    "\n",
    "assigned_poses = 0\n",
    "\n",
    "cluster_to_poses = {}\n",
    "for i, cluster_id in enumerate(labels):\n",
    "    if cluster_id not in cluster_to_poses:\n",
    "        cluster_to_poses[cluster_id] = [i]\n",
    "    else:\n",
    "        cluster_to_poses[cluster_id].append(i)\n",
    "        \n",
    "# Build an alternative, filtered movelet set that is\n",
    "# filtered down to just one movelet per track in a cluster\n",
    "# i.e., when more than one pose per track is in a given\n",
    "# cluster, just keep the first one. This has the effect\n",
    "# of stripping out repeated poses that are part of the\n",
    "# same low-motion movelet.\n",
    "\n",
    "filtered_movelet_indices = []\n",
    "\n",
    "tracks_per_cluster = []\n",
    "poses_per_track_per_cluster = []\n",
    "        \n",
    "for cluster_id in range(-1, max(labels) + 1):\n",
    "    # print(\"Poses in cluster\", cluster_id, labels.count(cluster_id))\n",
    "\n",
    "    cluster_track_poses = {}\n",
    "    for movelet_id in cluster_to_poses[cluster_id]:\n",
    "        movelet_track = frozen_movelets.iloc[movelet_id]['track_id']\n",
    "        if movelet_track not in cluster_track_poses:\n",
    "            if cluster_id != -1:\n",
    "                filtered_movelet_indices.append(movelet_id)\n",
    "            cluster_track_poses[movelet_track] = 1 # Include non-clustered poses?\n",
    "        else:\n",
    "            cluster_track_poses[movelet_track] += 1\n",
    "            \n",
    "    if cluster_id != -1:\n",
    "        assigned_poses += labels.count(cluster_id)\n",
    "        tracks_per_cluster.append(len(cluster_track_poses))\n",
    "        poses_per_track_per_cluster.append(labels.count(cluster_id) / len(cluster_track_poses))\n",
    "    \n",
    "    # print(\"Tracks in cluster\", cluster_id, len(cluster_track_poses))\n",
    "\n",
    "print(\"assigned\", assigned_poses, \"poses out of\", len(labels), round(assigned_poses/len(labels),4))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,4))\n",
    "ax = fig2.gca()\n",
    "n, bins, patches = plt.hist(tracks_per_cluster, bins=20)\n",
    "ax.set_title(\"Tracks per cluster\")\n",
    "ax.set_xlabel(\"# Tracks\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()\n",
    "\n",
    "fig3 = plt.figure(figsize=(10,4))\n",
    "ax = fig3.gca()\n",
    "n, bins, patches = plt.hist(poses_per_track_per_cluster, bins=30)\n",
    "ax.set_title(\"Poses per track per cluster\")\n",
    "ax.set_xlabel(\"Poses/track\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the full pose data for each representative movelet from a track in a cluster,\n",
    "# to be used to display armatures and source image excerpts in the cluster plot and\n",
    "# in visualizations of the cluster averages\n",
    "\n",
    "print(len(filtered_movelet_indices))\n",
    "\n",
    "filtered_movelet_counts = dict()\n",
    "for i in filtered_movelet_indices:\n",
    "    filtered_movelet_counts[i] = filtered_movelet_counts.get(i, 0) + 1\n",
    "\n",
    "print(\"Filtered movelets:\",len(set(filtered_movelet_indices)))\n",
    "filtered_movelets = frozen_movelets.iloc[list(set(filtered_movelet_indices))]\n",
    "filtered_movelets.reset_index(inplace=True)\n",
    "filtered_poses = filtered_movelets['norm'].tolist()\n",
    "filtered_poses = [np.nan_to_num(pose, nan=-1) for pose in filtered_poses]\n",
    "len(filtered_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"visualizing UMAP preclustered model\")\n",
    "show_poses = True\n",
    "plot_images = True\n",
    "\n",
    "if show_poses:\n",
    "    ord_cluster_to_poses = res = OrderedDict(sorted(cluster_to_poses.items(), key = lambda x : len(x[1]), reverse=True)).keys()\n",
    "    for cluster_id in ord_cluster_to_poses:\n",
    "        cluster_poses = []\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(UPSCALE * 100 / fig.dpi, UPSCALE * 100 / fig.dpi)\n",
    "        fig.canvas.draw()\n",
    "        print(\"CLUSTER:\", cluster_id, \"POSES:\", len(cluster_to_poses[cluster_id]))\n",
    "        for pose_index in cluster_to_poses[cluster_id]:\n",
    "            cl_pose = frozen_poses[pose_index]\n",
    "            cl_pose[cl_pose==-1] = np.nan\n",
    "            cluster_poses.append(cl_pose)\n",
    "        cluster_average = np.nanmean(np.array(cluster_poses), axis=0).tolist()\n",
    "        armature_prevalences = get_armature_prevalences(cluster_poses)\n",
    "        cluster_average = np.array_split(cluster_average, len(cluster_average) / 2)\n",
    "        #print(\"Average pose in cluster\", cluster_id, cluster_average)\n",
    "        cluster_average_img = draw_normalized_and_unflattened_pose(\n",
    "            cluster_average, armature_prevalences=armature_prevalences\n",
    "        )\n",
    "        #plt.figure(figsize=(2,2))\n",
    "        plt.imshow(cluster_average_img)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "fig = plt.figure(figsize=(40,40))\n",
    "ax = fig.gca()\n",
    "cm = colormaps[\"Spectral\"]\n",
    "norm = colors.Normalize(vmin=-1, vmax=max(labels))\n",
    "\n",
    "if plot_images:\n",
    "    \n",
    "    ax.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], alpha=0)\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cl_pose = frozen_poses[i]\n",
    "        cl_pose[cl_pose==-1] = np.nan\n",
    "        cluster_pose = np.array_split(cl_pose, len(cl_pose) / 2)\n",
    "        cluster_pose_img = draw_normalized_and_unflattened_pose(\n",
    "            cluster_pose, armature_prevalences=[1] * 19\n",
    "        )\n",
    "        #img = Image.fromarray(img_region)\n",
    "        img = cluster_pose_img\n",
    "        img.thumbnail((40, 40), resample=Image.Resampling.LANCZOS)\n",
    "        ab = AnnotationBbox(OffsetImage(np.asarray(img)), (clusterable_embedding[i, 0], clusterable_embedding[i, 1]), frameon=False)\n",
    "        #ab.patch.set_linewidth(0)\n",
    "        #ab.patch.set(color=cm(norm(cluster_id)))\n",
    "\n",
    "        ax.add_artist(ab)\n",
    "else:\n",
    "    ax.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_embedding = umap.UMAP(\n",
    "    random_state=42,\n",
    ").fit_transform(filtered_poses)\n",
    "\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterable_embedding = umap.UMAP(\n",
    "    n_neighbors=10,\n",
    "    min_dist=1.0,\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    ").fit_transform(filtered_poses)\n",
    "\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting clustering model\")\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=3, min_samples=4) # , max_cluster_size=15\n",
    "hdb.fit(filtered_poses)\n",
    "labels = hdb.labels_.tolist()\n",
    "\n",
    "assigned_poses = 0\n",
    "\n",
    "cluster_to_poses = {}\n",
    "for i, cluster_id in enumerate(labels):\n",
    "    if cluster_id not in cluster_to_poses:\n",
    "        cluster_to_poses[cluster_id] = [i]\n",
    "    else:\n",
    "        cluster_to_poses[cluster_id].append(i)\n",
    "\n",
    "tracks_per_cluster = []\n",
    "poses_per_track_per_cluster = []\n",
    "        \n",
    "for cluster_id in range(-1, max(labels) + 1):\n",
    "    # print(\"Poses in cluster\", cluster_id, labels.count(cluster_id))\n",
    "\n",
    "    cluster_track_poses = {}\n",
    "    for movelet_id in cluster_to_poses[cluster_id]:\n",
    "        movelet_track = filtered_movelets.iloc[movelet_id]['track_id']\n",
    "        if movelet_track not in cluster_track_poses:\n",
    "            cluster_track_poses[movelet_track] = 1\n",
    "        else:\n",
    "            cluster_track_poses[movelet_track] += 1\n",
    "            \n",
    "    if cluster_id != -1:\n",
    "        assigned_poses += labels.count(cluster_id)\n",
    "        tracks_per_cluster.append(len(cluster_track_poses))\n",
    "        poses_per_track_per_cluster.append(labels.count(cluster_id) / len(cluster_track_poses))\n",
    "    \n",
    "    # print(\"Tracks in cluster\", cluster_id, len(cluster_track_poses))\n",
    "\n",
    "print(\"assigned\", assigned_poses, \"poses out of\", len(labels), round(assigned_poses/len(labels),4))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,4))\n",
    "ax = fig2.gca()\n",
    "n, bins, patches = plt.hist(tracks_per_cluster, bins=20)\n",
    "ax.set_title(\"Tracks per cluster\")\n",
    "ax.set_xlabel(\"# Tracks\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()\n",
    "\n",
    "fig3 = plt.figure(figsize=(10,4))\n",
    "ax = fig3.gca()\n",
    "n, bins, patches = plt.hist(poses_per_track_per_cluster, bins=30)\n",
    "ax.set_title(\"Poses per track per cluster\")\n",
    "ax.set_xlabel(\"Poses/track\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting UMAP preclustered model\")\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=3, min_samples=4) # , max_cluster_size=15\n",
    "hdb.fit(clusterable_embedding)\n",
    "labels = hdb.labels_.tolist()\n",
    "\n",
    "assigned_poses = 0\n",
    "\n",
    "cluster_to_poses = {}\n",
    "for i, cluster_id in enumerate(labels):\n",
    "    if cluster_id not in cluster_to_poses:\n",
    "        cluster_to_poses[cluster_id] = [i]\n",
    "    else:\n",
    "        cluster_to_poses[cluster_id].append(i)\n",
    "        \n",
    "# Build an alternative, filtered movelet set that is\n",
    "# filtered down to just one movelet per track in a cluster\n",
    "# i.e., when more than one pose per track is in a given\n",
    "# cluster, just keep the first one. This has the effect\n",
    "# of stripping out repeated poses that are part of the\n",
    "# same low-motion movelet.\n",
    "\n",
    "filtered_movelet_indices = []\n",
    "\n",
    "tracks_per_cluster = []\n",
    "poses_per_track_per_cluster = []\n",
    "        \n",
    "for cluster_id in range(-1, max(labels) + 1):\n",
    "    # print(\"Poses in cluster\", cluster_id, labels.count(cluster_id))\n",
    "\n",
    "    cluster_track_poses = {}\n",
    "    for movelet_id in cluster_to_poses[cluster_id]:\n",
    "        movelet_track = filtered_movelets.iloc[movelet_id]['track_id']\n",
    "        if movelet_track not in cluster_track_poses:\n",
    "            if cluster_id != -1:\n",
    "                filtered_movelet_indices.append(movelet_id)\n",
    "            cluster_track_poses[movelet_track] = 1 # Include non-clustered poses?\n",
    "        else:\n",
    "            cluster_track_poses[movelet_track] += 1\n",
    "            \n",
    "    if cluster_id != -1:\n",
    "        assigned_poses += labels.count(cluster_id)\n",
    "        tracks_per_cluster.append(len(cluster_track_poses))\n",
    "        poses_per_track_per_cluster.append(labels.count(cluster_id) / len(cluster_track_poses))\n",
    "    \n",
    "    # print(\"Tracks in cluster\", cluster_id, len(cluster_track_poses))\n",
    "\n",
    "print(\"assigned\", assigned_poses, \"poses out of\", len(labels), round(assigned_poses/len(labels),4))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,4))\n",
    "ax = fig2.gca()\n",
    "n, bins, patches = plt.hist(tracks_per_cluster, bins=20)\n",
    "ax.set_title(\"Tracks per cluster\")\n",
    "ax.set_xlabel(\"# Tracks\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()\n",
    "\n",
    "fig3 = plt.figure(figsize=(10,4))\n",
    "ax = fig3.gca()\n",
    "n, bins, patches = plt.hist(poses_per_track_per_cluster, bins=30)\n",
    "ax.set_title(\"Poses per track per cluster\")\n",
    "ax.set_xlabel(\"Poses/track\")\n",
    "ax.set_ylabel(\"# Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"visualizing UMAP preclustered model\")\n",
    "show_poses = False\n",
    "plot_images = False\n",
    "save_images = True\n",
    "\n",
    "if show_poses:\n",
    "    ord_cluster_to_poses = res = OrderedDict(sorted(cluster_to_poses.items(), key = lambda x : len(x[1]), reverse=True)).keys()\n",
    "    for cluster_id in ord_cluster_to_poses:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(UPSCALE * 100 / fig.dpi, UPSCALE * 100 / fig.dpi)\n",
    "        fig.canvas.draw()\n",
    "        \n",
    "        cluster_poses = []\n",
    "        print(\"CLUSTER:\", cluster_id, \"POSES:\", len(cluster_to_poses[cluster_id]))\n",
    "        for pose_index in cluster_to_poses[cluster_id]:\n",
    "            cl_pose = filtered_poses[pose_index]\n",
    "            cl_pose[cl_pose==-1] = np.nan\n",
    "            cluster_poses.append(cl_pose)\n",
    "        cluster_average = np.nanmean(np.array(cluster_poses), axis=0).tolist()\n",
    "        armature_prevalences = get_armature_prevalences(cluster_poses)\n",
    "        cluster_average = np.array_split(cluster_average, len(cluster_average) / 2)\n",
    "        #print(\"Average pose in cluster\", cluster_id, cluster_average)\n",
    "        cluster_average_img = draw_normalized_and_unflattened_pose(\n",
    "            cluster_average, armature_prevalences=armature_prevalences\n",
    "        )\n",
    "        #plt.figure(figsize=(2,2))\n",
    "        plt.imshow(cluster_average_img)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "ax = fig.gca()\n",
    "cm = colormaps[\"Spectral\"]\n",
    "norm = colors.Normalize(vmin=-1, vmax=max(labels))\n",
    "\n",
    "if save_images:\n",
    "    images_dir = f\"pose_images/{video_name}\"\n",
    "    if not os.path.isdir(\"pose_images\"):\n",
    "        os.mkdir(\"pose_images\")\n",
    "    if not os.path.isdir(images_dir):\n",
    "        os.mkdir(images_dir)\n",
    "    img_metadata_file = open(f\"{video_name}.csv\", \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    # PixPlot metadata elements: \n",
    "    # year is an integer but doesn't need to be a year\n",
    "    # label can be the cluster the pose is in\n",
    "    # description is plain text\n",
    "    # can also supply any number of \"tags\", but it's not clear how these would be useful\n",
    "    img_metadata_file.write(\",\".join([\"filename\", \"description\", \"year\", \"label\"]) + \"\\n\")\n",
    "    \n",
    "    features_dir = f\"pose_features/{video_name}\"\n",
    "    if not os.path.isdir(\"pose_features\"):\n",
    "        os.mkdir(\"pose_features\")\n",
    "    if not os.path.isdir(features_dir):\n",
    "        os.mkdir(features_dir)\n",
    "    \n",
    "\n",
    "if plot_images or save_images:\n",
    "    ax.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], alpha=0)\n",
    "\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        #if cluster_id == -1:\n",
    "        #    continue\n",
    "\n",
    "        # Use this code block if we want to draw the normalized pose anywhere\n",
    "#         cl_pose = filtered_poses[i]\n",
    "#         cl_pose[cl_pose==-1] = np.nan\n",
    "#         cluster_pose = np.array_split(cl_pose, len(cl_pose) / 2)\n",
    "#         cluster_pose_img = draw_normalized_and_unflattened_pose(\n",
    "#             cluster_pose, armature_prevalences=[1] * 19\n",
    "#         )\n",
    "\n",
    "        cluster_movelet = filtered_movelets.iloc[i]\n",
    "        # Prefer a target frame in the middle of the movelet, but if the actual pose index\n",
    "        # is missing from this frame (which can happen sometimes), just use the first frame\n",
    "        # of the movelet\n",
    "        try:\n",
    "            target_frame = round((cluster_movelet['end_frame'] + cluster_movelet['start_frame']) / 2)\n",
    "            target_movelet = poses_df[(poses_df['frame'] == target_frame) & (poses_df['track_id'] == cluster_movelet['track_id'])]\n",
    "            if len(target_movelet) == 0:\n",
    "                target_frame = cluster_movelet['start_frame']\n",
    "                target_movelet = poses_df[(poses_df['frame'] == target_frame) & (poses_df['track_id'] == cluster_movelet['track_id'])]\n",
    "            target_pose = poses_df[(poses_df['frame'] == target_frame) & (poses_df['track_id'] == cluster_movelet['track_id'])].iloc[0]\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't find representative pose from movelet middle or beginning, skipping\")\n",
    "            continue\n",
    "\n",
    "        save_name = f\"{images_dir}/{target_frame}_{target_pose['pose_idx']}.jpg\"\n",
    "\n",
    "        if not os.path.isfile(save_name):\n",
    "            \n",
    "            bbox = [round(v) for v in target_pose['bbox']]\n",
    "            print(\"frame\", target_frame, \"pose\", target_pose[\"pose_idx\"], \"pose bbox\", bbox)\n",
    "\n",
    "            frame_faces = await db.get_frame_faces(video_id, target_frame)\n",
    "\n",
    "            if len(frame_faces):\n",
    "                faces_df = pd.DataFrame.from_records(frame_faces, columns=frame_faces[0].keys())\n",
    "\n",
    "                target_face = faces_df[faces_df['pose_idx'] == target_pose['pose_idx']]\n",
    "                if (len(target_face)):\n",
    "                    target_face_df = target_face.iloc[0]\n",
    "                    face_bbox = [round(v) for v in target_face_df['bbox']]\n",
    "                    print(\"face_bbox\", face_bbox)\n",
    "\n",
    "                    min_x = min(bbox[0], face_bbox[0])\n",
    "                    min_y = min(bbox[1], face_bbox[1])\n",
    "                    max_x = max(bbox[0] + bbox[2], face_bbox[0] + face_bbox[2])\n",
    "                    max_y = max(bbox[1] + bbox[3], face_bbox[1] + face_bbox[3])\n",
    "                    b_w = max_x - min_x\n",
    "                    b_h = max_y - min_y\n",
    "\n",
    "                    # A bbox that includes the body and the face (if detected)\n",
    "                    bbox = [min_x, min_y, b_w, b_h]\n",
    "                    print(\"Combined bbox\", bbox)\n",
    "                \n",
    "            pose_frame_image = iio.imread(f\"/videos/{video_name}\", index=target_frame - 1, plugin=\"pyav\")\n",
    "\n",
    "            pose_img = Image.fromarray(pose_frame_image)\n",
    "\n",
    "            img_size = pose_img.size\n",
    "\n",
    "            pose_img = pose_img.resize((img_size[0] * UPSCALE, img_size[1] * UPSCALE))\n",
    "            drawing = ImageDraw.Draw(pose_img)\n",
    "            keypoints_triples = [(target_pose['keypoints'][i], target_pose['keypoints'][i+1], target_pose['keypoints'][i+2]) for i in range(0, len(target_pose['keypoints']), 3)]\n",
    "            drawing = draw_armatures(keypoints_triples, drawing)\n",
    "\n",
    "            pose_img = pose_img.resize(\n",
    "                (img_size[0], img_size[1]), resample=Image.Resampling.LANCZOS\n",
    "            )\n",
    "\n",
    "            cropped_pose_frame_image = pose_img.crop([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])\n",
    "\n",
    "            img = cropped_pose_frame_image\n",
    "\n",
    "            print(\"saving image\", save_name)\n",
    "            img.save(save_name)\n",
    "            \n",
    "            cropped_pose_frame_image.close()\n",
    "            pose_img.close()\n",
    "\n",
    "            # Assume there's always a features file if there's an image file\n",
    "            img_features = filtered_poses[i]\n",
    "            np.save(f\"{features_dir}/{target_frame}_{target_pose['pose_idx']}.npy\", img_features)\n",
    "            \n",
    "        else:\n",
    "            img = Image.open(save_name)\n",
    "            \n",
    "        frame_minute = round(target_frame / video_fps / 60)\n",
    "        \n",
    "        img_metadata_file.write(\",\".join([f\"{target_frame}_{target_pose['pose_idx']}.jpg\", f\"Frame {target_frame} | pose {target_pose['pose_idx']} | track {target_pose['track_id']}\", str(frame_minute), str(cluster_id)]) + \"\\n\")\n",
    "\n",
    "        if plot_images:\n",
    "            img.thumbnail((100, 100), resample=Image.Resampling.LANCZOS)\n",
    "            ab = AnnotationBbox(OffsetImage(np.asarray(img)), (clusterable_embedding[i, 0], clusterable_embedding[i, 1]), frameon=False)\n",
    "            #ab.patch.set_linewidth(0)\n",
    "            #ab.patch.set(color=cm(norm(cluster_id)))\n",
    "            ax.add_artist(ab)\n",
    "            ax.text(clusterable_embedding[i,0], clusterable_embedding[i, 1], cluster_id, color=\"red\") \n",
    "\n",
    "        img.close()\n",
    "\n",
    "    img_metadata_file.close()\n",
    "\n",
    "else:\n",
    "    ax.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_keep = set()\n",
    "\n",
    "with open(f\"{video_name}.csv\", \"r\", encoding=\"utf-8\") as img_metadata_file:\n",
    "    for la in img_metadata_file:\n",
    "        img_fn = la.strip().split(\",\")[0]\n",
    "        if img_fn == \"filename\":\n",
    "            continue\n",
    "        imgs_to_keep.add(img_fn)\n",
    "\n",
    "print(len(imgs_to_keep),\"unique images in metadata file\")\n",
    "        \n",
    "for fn in os.listdir(images_dir):\n",
    "    if os.path.isfile(f\"{images_dir}/{fn}\"):\n",
    "        if fn not in imgs_to_keep:\n",
    "            print(\"Deleting image\", fn)\n",
    "            os.unlink(f\"{images_dir}/{fn}\")\n",
    "            os.unlink(f\"{features_dir}/{fn.replace('jpg', 'npy')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
