{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import imageio.v3 as iio\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn import metrics\n",
    "from matplotlib import colormaps, colors\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import skew, skewtest, kurtosis, kurtosistest\n",
    "import umap\n",
    "\n",
    "from mime_db import MimeDb\n",
    "#from pose_functions import *\n",
    "\n",
    "MAX_PIXEL_MOVEMENT = 500 # Disregard movelets with > this many pixels movement (probably necessary)\n",
    "MAX_3D_MOVEMENT = 10 # in meters/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DB search indices for POEM embeddings and action embeddings\n",
    "# Only needs to be run once (unless the data changes)\n",
    "await db.assign_poem_embeddings([], reindex=True)\n",
    "query = \"\"\"\n",
    "            CREATE INDEX ON pose\n",
    "            USING ivfflat (ava_action vector_cosine_ops)\n",
    "            ;\n",
    "            \"\"\"\n",
    "await db._pool.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_stats(distrib, plot=False):\n",
    "\n",
    "    if len(distrib) == 0:\n",
    "        return {\"count\": 0, \"mean\": 0, \"median\": 0, \"stdev\": 0, \"skewness\": 0, \"kurtosis\": 0}\n",
    "\n",
    "    if skewtest(distrib).pvalue < .05:\n",
    "        skewness = skew(distrib)\n",
    "    else:\n",
    "        skewness = 0\n",
    "\n",
    "    if kurtosistest(distrib).pvalue < .05:\n",
    "        kurtosis_value = kurtosis(distrib)\n",
    "    else:\n",
    "        kurtosis_value = 0\n",
    "\n",
    "    if plot:\n",
    "        plt.hist(distrib, bins=\"auto\")  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "\n",
    "    return {\"count\": len(distrib), \"mean\": np.mean(distrib), \"median\": np.median(distrib), \"stdev\": np.std(distrib), \"skewness\": skewness, \"kurtosis\": kurtosis_value}\n",
    "\n",
    "\n",
    "def get_body_units(keypoints3d, camera):\n",
    "    return get_body_unit(project_pose(keypoints3d, camera))\n",
    "\n",
    "\n",
    "def get_body_unit(pose_proj):\n",
    "    shoulders = euclidean(pose_proj[1], pose_proj[2])\n",
    "    left_upper_arm = euclidean(pose_proj[1], pose_proj[3])\n",
    "    right_upper_arm = euclidean(pose_proj[2], pose_proj[4])\n",
    "    left_forearm = euclidean(pose_proj[3], pose_proj[5])\n",
    "    right_forearm = euclidean(pose_proj[4], pose_proj[6])\n",
    "    hips = euclidean(pose_proj[7], pose_proj[8])\n",
    "    left_thigh = euclidean(pose_proj[7], pose_proj[9])\n",
    "    right_thigh = euclidean(pose_proj[8], pose_proj[10])\n",
    "    left_lower_leg = euclidean(pose_proj[9], pose_proj[11])\n",
    "    right_lower_leg = euclidean(pose_proj[10], pose_proj[12])\n",
    "\n",
    "    return np.mean([shoulders, left_upper_arm, right_upper_arm, left_forearm, right_forearm, hips, left_thigh, right_thigh, left_lower_leg, right_lower_leg])\n",
    "\n",
    "\n",
    "def project_pose(keypoints3d, camera):\n",
    "    kp_array = np.array(np.split(keypoints3d, 13))\n",
    "    kp_camera = np.array(camera)\n",
    "\n",
    "    return kp_array + kp_camera.T\n",
    "\n",
    "\n",
    "def get_projected_pose_centroid(keypoints3d, camera):\n",
    "\n",
    "    #proj_centroid = np.mean(kp_array, axis=0) + kp_camera.T\n",
    "    proj_centroid = np.mean(project_pose(keypoints3d, camera), axis=0)\n",
    "\n",
    "    proj_centroid[2] = proj_centroid[2]/200\n",
    "\n",
    "    return proj_centroid\n",
    "\n",
    "\n",
    "def get_projected_pose_bbox(keypoints3d, camera):\n",
    "    ppose = project_pose(keypoints3d, camera)\n",
    "    ppose[:,2] /= 200\n",
    "    mins = np.min(ppose, axis=0).tolist()\n",
    "    maxes = np.max(ppose, axis=0).tolist()\n",
    "    \n",
    "    return [mins, maxes]\n",
    "\n",
    "\n",
    "def are_overlapping(bbox1, bbox2):\n",
    "    # Each bbox is [[xmin, ymin, zmin], [xmax, ymax, zmax]]\n",
    "\n",
    "    return ((bbox1[1][0] >= bbox2[0][0] and bbox1[1][0] <= bbox2[1][0]) \\\n",
    "            or (bbox1[0][0] <= bbox2[1][0] and bbox1[0][0] >= bbox2[0][0])) \\\n",
    "            and ((bbox1[1][1] >= bbox2[0][1] and bbox1[1][1] <= bbox2[1][1]) \\\n",
    "            or (bbox1[0][1] <= bbox2[1][1] and bbox1[0][1] >= bbox2[0][1])) \\\n",
    "            and ((bbox1[1][2] >= bbox2[0][2] and bbox1[1][2] <= bbox2[1][2]) \\\n",
    "            or (bbox1[0][2] <= bbox2[1][2] and bbox1[0][2] >= bbox2[0][2]))\n",
    "\n",
    "    # From https://blender.stackexchange.com/questions/253355/collision-detection\n",
    "    # isColliding = ((x_max >= x_min2 and x_max <= x_max2) \\\n",
    "    #                 or (x_min <= x_max2 and x_min >= x_min2)) \\\n",
    "    #                 and ((y_max >= y_min2 and y_max <= y_max2) \\\n",
    "    #                 or (y_min <= y_max2 and y_min >= y_min2)) \\\n",
    "    #                 and ((z_max >= z_min2 and z_max <= z_max2) \\\n",
    "    #                 or (z_min <= z_max2 and z_min >= z_min2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory processin of a single video (for testing only, usually this cell is skipped)\n",
    "\n",
    "VIDEO_FILE = \"A_Letter_to_My_Nephew.mp4\" # Just the name of the video file, no path\n",
    "\n",
    "#db = await MimeDb.create()\n",
    "\n",
    "video_path = Path(\"videos\", VIDEO_FILE)\n",
    "\n",
    "# Connect to the database\n",
    "\n",
    "# Get video metadata\n",
    "video_name = video_path.name\n",
    "print(video_name)\n",
    "video_id = await db.get_video_id(video_name)\n",
    "\n",
    "print(\"VIDEO ID\", video_id)\n",
    "\n",
    "video_data = await db.get_video_by_id(video_id)\n",
    "video_fps = video_data[\"fps\"]\n",
    "video_frame_count = video_data[\"frame_count\"]\n",
    "\n",
    "video_seconds = video_frame_count / video_fps\n",
    "\n",
    "video_movelets = await db.get_movelet_data_from_video(video_id)\n",
    "movelets_df = pd.DataFrame.from_records(video_movelets, columns=video_movelets[0].keys())\n",
    "\n",
    "video_poses = await db.get_pose_data_from_video(video_id)\n",
    "\n",
    "video_shots = await db.get_video_shots(video_id)\n",
    "shots_df = pd.DataFrame.from_records(video_shots, columns=video_shots[0].keys())\n",
    "\n",
    "poses_df = pd.DataFrame.from_records(video_poses, columns=video_poses[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = await MimeDb.create()\n",
    "\n",
    "movelet_poem_embeds_by_video = []\n",
    "global_3d_coco_13_by_video = []\n",
    "action_embeds_by_video = []\n",
    "\n",
    "async def get_video_stats(video_file):\n",
    "\n",
    "    video_path = Path(\"videos\", video_file)\n",
    "\n",
    "    # Connect to the database\n",
    "\n",
    "    # Get video metadata\n",
    "    video_name = video_path.name\n",
    "    print(video_name)\n",
    "    video_id = await db.get_video_id(video_name)\n",
    "\n",
    "    print(\"VIDEO ID\", video_id)\n",
    "\n",
    "    video_data = await db.get_video_by_id(video_id)\n",
    "    video_fps = video_data[\"fps\"]\n",
    "    video_frame_count = video_data[\"frame_count\"]\n",
    "\n",
    "    video_seconds = video_frame_count / video_fps\n",
    "\n",
    "    print(\"GETTING MOVELET DATA\")\n",
    "\n",
    "    video_movelets = await db.get_movelet_data_from_video(video_id)\n",
    "    movelets_df = pd.DataFrame.from_records(video_movelets, columns=video_movelets[0].keys())\n",
    "\n",
    "    print(\"GETTING POSE DATA\")\n",
    "\n",
    "    video_poses = await db.get_pose_data_from_video(video_id)\n",
    "\n",
    "    print(\"GETTING SHOTS DATA\")\n",
    "\n",
    "    video_shots = await db.get_video_shots(video_id)\n",
    "    shots_df = pd.DataFrame.from_records(video_shots, columns=video_shots[0].keys())\n",
    "\n",
    "    poses_df = pd.DataFrame.from_records(video_poses, columns=video_poses[0].keys())\n",
    "\n",
    "    video_3d_coco13_globals = np.array(poses_df[\"global3d_coco13\"].to_list()) # DataFrame\n",
    "    mean_3d_coco13_global = video_3d_coco13_globals.mean(axis=0).tolist() # ndarray\n",
    "    median_3d_coco13_global = np.median(video_3d_coco13_globals, axis=0).tolist() # ndarray\n",
    "\n",
    "    video_poem_embeddings = np.array(poses_df[\"poem_embedding\"].to_list()) # DataFrame\n",
    "    mean_poem_embedding = video_poem_embeddings.mean(axis=0).tolist() # ndarray\n",
    "    median_poem_embedding = np.median(video_poem_embeddings, axis=0).tolist() # ndarray\n",
    "\n",
    "    video_action_embeddings = np.array(poses_df[\"ava_action\"].to_list()) # DataFrame\n",
    "    mean_action_embedding = video_action_embeddings.mean(axis=0).tolist() # ndarray\n",
    "    median_action_embedding = np.median(video_action_embeddings, axis=0).tolist() # ndarray\n",
    "\n",
    "    print(\"TOTAL MOVELETS:\", len(movelets_df))\n",
    "    print(\"NON-MOTION MOVELETS:\", len(movelets_df[movelets_df['movement'].isna()]))\n",
    "\n",
    "    print(\"NON-3D-MOTION MOVELETS:\", len(movelets_df[movelets_df['movement3d'].isna()]))\n",
    "\n",
    "    print(\"MOVELETS WITH STILL MOTION:\", len(movelets_df[movelets_df['movement'] == 0]))\n",
    "\n",
    "    print(\"MOVELETS WITH STILL 3D MOTION:\", len(movelets_df[movelets_df['movement3d'] == 0]))\n",
    "\n",
    "    print(\"MOVELETS WITH MOVEMENT < 10px/sec:\", len(movelets_df[(movelets_df['movement'] >= 0) & (movelets_df['movement'] < 10)]))\n",
    "\n",
    "    print(\"MEAN MOVEMENT PER MOVELET (norm px/sec):\", np.nanmean(movelets_df['movement']))\n",
    "    print(\"MEDIAN MOVEMENT PER MOVELET (norm px/sec):\", np.nanmedian(movelets_df['movement']))\n",
    "\n",
    "    print(\"MEAN 3D MOVEMENT PER MOVELET (meters/sec):\", np.nanmean(movelets_df['movement3d']))\n",
    "    print(\"MEDIAN 3D MOVEMENT PER MOVELET (meters/sec):\", np.nanmedian(movelets_df['movement3d']))\n",
    "\n",
    "    nonnull_movelets_df = movelets_df.copy()\n",
    "    nonnull_movelets_df['movement'] = nonnull_movelets_df['movement'].fillna(-1)\n",
    "\n",
    "    movement_distribution = nonnull_movelets_df[nonnull_movelets_df['movement'] <= MAX_PIXEL_MOVEMENT]['movement']\n",
    "\n",
    "    n, bins, patches = plt.hist(movement_distribution, bins=300)\n",
    "    plt.xlabel(\"Movement (normalized pixels/sec)\")\n",
    "    plt.ylabel(\"# Movelets\")\n",
    "    top_bin = n[1:].argmax()\n",
    "    print('most frequent movement bin: (' + str(bins[top_bin]) + ',' + str(bins[top_bin+1]) + ')')\n",
    "    print('movement mode: '+ str((bins[top_bin] + bins[top_bin+1])/2))\n",
    "    movement_mode = (bins[top_bin] + bins[top_bin+1])/2\n",
    "    plt.show()\n",
    "\n",
    "    movement_distribution_3d = nonnull_movelets_df[nonnull_movelets_df['movement3d'] <= MAX_3D_MOVEMENT]['movement3d']\n",
    "\n",
    "    n, bins, patches = plt.hist(movement_distribution_3d, bins=300)\n",
    "    plt.xlabel(\"3D Movement (meters/sec)\")\n",
    "    plt.ylabel(\"# Movelets\")\n",
    "    top_bin = n[1:].argmax()\n",
    "    print('most frequent 3D movement bin: (' + str(bins[top_bin]) + ',' + str(bins[top_bin+1]) + ')')\n",
    "    print('3D movement mode: '+ str((bins[top_bin] + bins[top_bin+1])/2))\n",
    "    movement_mode_3d = (bins[top_bin] + bins[top_bin+1])/2\n",
    "    plt.show()\n",
    "\n",
    "    movelet_poem_embeds_by_video.append(nonnull_movelets_df.poem_embedding.to_list())\n",
    "\n",
    "    global_3d_coco_13_by_video.append(video_3d_coco13_globals)\n",
    "\n",
    "    action_embeds_by_video.append(video_action_embeddings)\n",
    "\n",
    "    # Maybe use frozen_poses to compare pose distributions? Or just use all poses in the performance?\n",
    "    #frozen_movelets = movelets_df[(movelets_df['movement'] >= 0) & (movelets_df['movement'] < movement_mode)].reset_index()\n",
    "    #frozen_poses = frozen_movelets['norm'].tolist()\n",
    "\n",
    "    poses_df = poses_df.merge(shots_df, left_on=\"frame\", right_on=\"frame\")\n",
    "\n",
    "    poses_by_frame = poses_df.groupby(\"frame\")\n",
    "\n",
    "    all_distances = []\n",
    "    all_mean_distances_per_frame = []\n",
    "    poses_per_populated_frame = []\n",
    "    overlaps_in_frame = []\n",
    "\n",
    "    for frame, frame_poses_df in poses_by_frame:\n",
    "\n",
    "        #print(\"FRAME\", frame, \"SHOT\", frame_poses_df.iloc[0][\"shot\"], \"POSES\", len(frame_poses_df))\n",
    "        \n",
    "        poses_per_populated_frame.append(len(frame_poses_df))\n",
    "\n",
    "        if len(frame_poses_df) <= 1:\n",
    "            continue\n",
    "\n",
    "        pose_centroids = frame_poses_df.apply(lambda x: get_projected_pose_centroid(x.keypoints3d, x.camera), axis=1)\n",
    "        #print(\"POSE CENTROIDS\")\n",
    "        pose_centroids = [list(centroid) for centroid in pose_centroids]\n",
    "        #print(pose_centroids)\n",
    "\n",
    "        dist_matrix = distance_matrix(pose_centroids, pose_centroids)\n",
    "\n",
    "        #print(\"DISTANCE MATRIX\")\n",
    "        #print(dist_matrix)\n",
    "\n",
    "        upper_diagonal = list(dist_matrix[np.triu_indices(len(dist_matrix), k=1)])\n",
    "\n",
    "        #print(\"UPPER DIAGONAL\")\n",
    "        #print(upper_diagonal)\n",
    "\n",
    "        mean_interpose_distance = np.mean(upper_diagonal)\n",
    "\n",
    "        all_distances.extend(upper_diagonal)\n",
    "        all_mean_distances_per_frame.append(mean_interpose_distance)\n",
    "\n",
    "        #if len(pose_centroids) > 1:\n",
    "        #    print(\"EUCLIDEAN\")\n",
    "        #    print(euclidean(pose_centroids[0], pose_centroids[1]))\n",
    "\n",
    "        pose_bboxes = frame_poses_df.apply(lambda x: get_projected_pose_bbox(x.keypoints3d, x.camera), axis=1)\n",
    "        pose_bboxes = [list(bbox) for bbox in pose_bboxes]\n",
    "\n",
    "        #print(\"POSE BOUNDING BOXES\")\n",
    "        #print(pose_bboxes)\n",
    "\n",
    "        total_overlaps = 0\n",
    "        for i, bbox1 in enumerate(pose_bboxes):\n",
    "            for j, bbox2 in enumerate(pose_bboxes):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                if are_overlapping(bbox1, bbox2):\n",
    "                    #print(\"OVERLAP DETECTED IN FRAME\", frame_poses_df.iloc[0][\"frame\"], \"SHOT\", frame_poses_df.iloc[0][\"shot\"], bbox1, bbox2)\n",
    "                    total_overlaps += 1\n",
    "\n",
    "        if total_overlaps > 1:\n",
    "            overlaps_in_frame.append(total_overlaps)\n",
    "\n",
    "        #pose_body_units = shot_poses_df.apply(lambda x: get_body_units(x.keypoints3d, x.camera), axis=1)\n",
    "        #print(\"POSE BODY UNITS\")\n",
    "        #print(pose_body_units)\n",
    "\n",
    "        #print(shot_poses_df[\"camera\"]) # camera pos is relative to *each* detection\n",
    "        #if shot > 101:\n",
    "        #    break\n",
    "\n",
    "\n",
    "    print(\"POSES PER POPULATED FRAME:\")\n",
    "    poses_ppf_stats = get_distribution_stats(poses_per_populated_frame, False)\n",
    "\n",
    "    print(\"OVERLAPS (# POSES OVERLAPPING):\")\n",
    "    pose_overlaps_stats = get_distribution_stats(overlaps_in_frame, False)\n",
    "\n",
    "    print(\"MEAN INTERPOSE DISTANCES PER POPULATED FRAME:\")\n",
    "    interpose_dist_ppf_stats = get_distribution_stats(all_mean_distances_per_frame, False)\n",
    "\n",
    "    print(\"ALL INTERPOSE DISTANCES\")\n",
    "    interpose_dist_stats = get_distribution_stats(all_distances, False)\n",
    "\n",
    "\n",
    "    # Merge non-null movelets with poses to calculate sidereal movement per pose per frame|second\n",
    "\n",
    "    def get_sidereal_motion(movelet):\n",
    "\n",
    "        # There shouldn't ever be more than one pose centroid that matches these queries (right???)\n",
    "        start_centroid = poses_df[(poses_df[\"track_id\"] == movelet[\"track_id\"]) & (poses_df[\"frame\"] == movelet[\"start_frame\"])][\"centroid_3d\"].iloc[0]\n",
    "        end_centroid = poses_df[(poses_df[\"track_id\"] == movelet[\"track_id\"]) & (poses_df[\"frame\"] == movelet[\"end_frame\"])][\"centroid_3d\"].iloc[0]\n",
    "        \n",
    "        return euclidean(start_centroid, end_centroid)\n",
    "\n",
    "    movelets_df = nonnull_movelets_df[nonnull_movelets_df['movement'] <= MAX_PIXEL_MOVEMENT]\n",
    "\n",
    "    poses_df[\"centroid_3d\"] = poses_df.apply(lambda x: get_projected_pose_centroid(x.keypoints3d, x.camera), axis=1)\n",
    "\n",
    "    movelets_df[\"sidereal\"] = movelets_df.apply(get_sidereal_motion, axis=1)\n",
    "\n",
    "    # Sidereal motion per second, based on movelet duration\n",
    "\n",
    "    print(\"SIDEREAL MOTION PER MOVELET\")\n",
    "    sidereal_motion_stats = get_distribution_stats(movelets_df[movelets_df[\"sidereal\"] < 1][\"sidereal\"].tolist(), False)\n",
    "\n",
    "    poses_by_shot = poses_df.groupby(\"shot\")\n",
    "\n",
    "    tracks_per_shot = []\n",
    "\n",
    "    for shot, shot_poses_df in poses_by_shot:\n",
    "\n",
    "        total_tracks = shot_poses_df.track_id.unique()\n",
    "\n",
    "        if len(total_tracks):\n",
    "            tracks_per_shot.append(len(total_tracks))\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(\"TRACKS PER SHOT:\")\n",
    "    tracks_per_shot_stats = get_distribution_stats(tracks_per_shot, False)\n",
    "\n",
    "    print(\"NORMALIZED MOVEMENT STATS:\")\n",
    "    normalized_movement_stats = get_distribution_stats(movement_distribution, False)\n",
    "\n",
    "    print(\"3D MOVEMENT STATS:\")\n",
    "    movement_3d_stats = get_distribution_stats(movement_distribution_3d, False)\n",
    "\n",
    "    video_stats = {\"seconds\": video_seconds, \"mean_poem_embedding\": mean_poem_embedding, \"median_poem_embedding\": median_poem_embedding, \"mean_3d_global\": mean_3d_coco13_global, \"median_3d_global\": median_3d_coco13_global, \"mean_action_embedding\": mean_action_embedding, \"median_action_embedding\": median_action_embedding, \"poses_ppf\": poses_ppf_stats, \"overlaps\": pose_overlaps_stats, \"interpose_dist_ppf\": interpose_dist_ppf_stats, \"interpose_dist\": interpose_dist_stats, \"sidereal\": sidereal_motion_stats, \"movement\": normalized_movement_stats, \"movement3d\": movement_3d_stats, \"tracks_per_shot\": tracks_per_shot_stats}\n",
    "    return video_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Run extraction for all videos in the DB (or at least all 30 core videos)\n",
    "# Write the derived statistics to a CSV(?) file\n",
    "# Use the derived statistics to perform some kind of factor analysis\n",
    "# Maybe a Bayesian classifier w/feature importance evaluation?\n",
    "\n",
    "# For normalized 3D pose factor analysis\n",
    "# Convert each global 3d pose into normalized vector of some size, maybe 25 x 25 x 25\n",
    "# Maybe also try doing this with the Pr-VIPE 2.5D embeddings?\n",
    "# Count the number of times each element appears in a pose\n",
    "# Normalize all counts by the max in all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_videos = await db.get_available_videos()\n",
    "videos_df = pd.DataFrame.from_records(all_videos, columns=all_videos[0].keys())\n",
    "all_video_stats = {}\n",
    "\n",
    "video_names = [video_name for video_name in videos_df.video_name.tolist() if not video_name.startswith(\"Don_Giovanni-\")]\n",
    "\n",
    "# Recalculate all video stats\n",
    "#for video_name in video_names:\n",
    "#   all_video_stats[video_name] = await get_video_stats(video_name)\n",
    "\n",
    "print(video_names)\n",
    "\n",
    "import pickle\n",
    "movelet_poem_embeds_by_video = pickle.load(open(\"stylometry/poem_embeds_by_video.pkl\", \"rb\"))\n",
    "global_3d_coco_13_by_video = pickle.load(open(\"stylometry/global_3d_coco_13_by_video.pkl\", \"rb\"))\n",
    "action_embeds_by_video = pickle.load(open(\"stylometry/action_embeds_by_video.pkl\", \"rb\"))\n",
    "\n",
    "# Load cached stats\n",
    "all_video_stats = json.load(open(\"stylometry/video_stats.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Get stats for the final video(s) on the list\n",
    "#for v in range(30, len(video_names)):\n",
    "#    all_video_stats[video_names[v]] = await get_video_stats(video_names[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment video data with directors and save stats to files\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "director_videos = {}\n",
    "video_directors = {}\n",
    "director_videos[\"BillTJones\"] = []\n",
    "director_videos[\"Castellucci\"] = []\n",
    "director_videos[\"Warlikowski\"] = []\n",
    "director_videos[\"Other\"] = []\n",
    "director_ids = {\"BillTJones\": 0, \"Castellucci\": 1, \"Warlikowski\": 2, \"Other\": 3}\n",
    "\n",
    "video_slugs = [\n",
    "\"JLN\",\n",
    "\"JAD\",\n",
    "\"JQP\",\n",
    "\"JDM\",\n",
    "\"JFD\",\n",
    "\"JHD\",\n",
    "\"JPP\",\n",
    "\"JSP\",\n",
    "\"JST\",\n",
    "\"JWS\",\n",
    "\"CDF\",\n",
    "\"CDA\",\n",
    "\"CDG\",\n",
    "\"CGD\",\n",
    "\"CIN\",\n",
    "\"CPW\",\n",
    "\"CPU\",\n",
    "\"CRM\",\n",
    "\"CRG\",\n",
    "\"CMF\",\n",
    "\"WBC\",\n",
    "\"WDG\",\n",
    "\"WES\",\n",
    "\"WIT\",\n",
    "\"WM1\",\n",
    "\"WM2\",\n",
    "\"WMC\",\n",
    "\"WCH\",\n",
    "\"WTF\",\n",
    "\"WWB\",\n",
    "\"JAE\",\n",
    "]\n",
    "\n",
    "for i, video_name in enumerate(video_names):\n",
    "    if i < 10 or i == 30:\n",
    "        director_videos[\"BillTJones\"].append(video_name)\n",
    "        video_directors[video_name] = director_ids[\"BillTJones\"]\n",
    "    elif i >= 10 and i < 20:\n",
    "        director_videos[\"Castellucci\"].append(video_name)\n",
    "        video_directors[video_name] = director_ids[\"Castellucci\"]\n",
    "    elif i >= 20 and i < 30:\n",
    "        director_videos[\"Warlikowski\"].append(video_name)\n",
    "        video_directors[video_name] = director_ids[\"Warlikowski\"]\n",
    "    else:\n",
    "        director_videos[\"Other\"].append(video_name)\n",
    "        video_directors[video_name] = director_ids[\"Other\"]\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"stylometry\"):\n",
    "    os.mkdir(\"stylometry\")\n",
    "\n",
    "for video_name in all_video_stats:\n",
    "    if video_name.startswith(\"Don_Giovanni-\"):\n",
    "        continue\n",
    "\n",
    "    all_video_stats[video_name][\"director\"] = video_directors[video_name]\n",
    "\n",
    "json.dump(all_video_stats, open(\"stylometry/video_stats.json\", \"w\", encoding=\"utf-8\"), indent=4)\n",
    "\n",
    "#pickle.dump(movelet_poem_embeds_by_video, open(\"stylometry/poem_embeds_by_video.pkl\", \"wb\"))\n",
    "#pickle.dump(global_3d_coco_13_by_video, open(\"stylometry/global_3d_coco_13_by_video.pkl\", \"wb\"))\n",
    "#pickle.dump(action_embeds_by_video, open(\"stylometry/action_embeds_by_video.pkl\", \"wb\"))\n",
    "\n",
    "#movelet_poem_embeds_by_video = pickle.load(open(\"stylometry/poem_embeds_by_video.pkl\", \"rb\"))\n",
    "#global_3d_coco_13_by_video = pickle.load(open(\"stylometry/global_3d_coco_13_by_video.pkl\", \"rb\"))\n",
    "#action_embeds_by_video = pickle.load(open(\"stylometry/action_embeds_by_video.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group poem and action embeddings and 3D global poses by director for embedding analysis\n",
    "\n",
    "poem_embeddings_by_director = {}\n",
    "action_embeddings_by_director = {}\n",
    "global_3d_coco_13_by_director = {}\n",
    "\n",
    "poem_embeddings_by_director[\"BillTJones\"] = []\n",
    "poem_embeddings_by_director[\"Castellucci\"] = []\n",
    "poem_embeddings_by_director[\"Warlikowski\"] = []\n",
    "for poem_embeds in movelet_poem_embeds_by_video[:10]:\n",
    "    poem_embeddings_by_director[\"BillTJones\"].extend(poem_embeds)\n",
    "for poem_embeds in movelet_poem_embeds_by_video[10:20]:\n",
    "    poem_embeddings_by_director[\"Castellucci\"].extend(poem_embeds)\n",
    "for poem_embeds in movelet_poem_embeds_by_video[20:30]:\n",
    "    poem_embeddings_by_director[\"Warlikowski\"].extend(poem_embeds)\n",
    "poem_embeddings_by_director[\"BillTJones\"].extend(movelet_poem_embeds_by_video[30])\n",
    "\n",
    "billtjones_mean_poem_embedding = np.mean(poem_embeddings_by_director[\"BillTJones\"], axis=0)\n",
    "castellucci_mean_poem_embedding = np.mean(poem_embeddings_by_director[\"Castellucci\"], axis=0)\n",
    "warlikowski_mean_poem_embedding = np.mean(poem_embeddings_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "billtjones_median_poem_embedding = np.median(poem_embeddings_by_director[\"BillTJones\"], axis=0)\n",
    "castellucci_median_poem_embedding = np.median(poem_embeddings_by_director[\"Castellucci\"], axis=0)\n",
    "warlikowski_median_poem_embedding = np.median(poem_embeddings_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "action_embeddings_by_director[\"BillTJones\"] = []\n",
    "action_embeddings_by_director[\"Castellucci\"] = []\n",
    "action_embeddings_by_director[\"Warlikowski\"] = []\n",
    "for action_embeds in action_embeds_by_video[:10]:\n",
    "    action_embeddings_by_director[\"BillTJones\"].extend(action_embeds)\n",
    "for action_embeds in action_embeds_by_video[10:20]:\n",
    "    action_embeddings_by_director[\"Castellucci\"].extend(action_embeds)\n",
    "for action_embeds in action_embeds_by_video[20:30]:\n",
    "    action_embeddings_by_director[\"Warlikowski\"].extend(action_embeds)\n",
    "action_embeddings_by_director[\"BillTJones\"].extend(action_embeds_by_video[30])\n",
    "\n",
    "billtjones_mean_action_embedding = np.mean(action_embeddings_by_director[\"BillTJones\"], axis=0)\n",
    "castellucci_mean_action_embedding = np.mean(action_embeddings_by_director[\"Castellucci\"], axis=0)\n",
    "warlikowski_mean_action_embedding = np.mean(action_embeddings_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "billtjones_median_action_embedding = np.median(action_embeddings_by_director[\"BillTJones\"], axis=0)\n",
    "castellucci_median_action_embedding = np.median(action_embeddings_by_director[\"Castellucci\"], axis=0)\n",
    "warlikowski_median_action_embedding = np.median(action_embeddings_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "global_3d_coco_13_by_director[\"BillTJones\"] = []\n",
    "global_3d_coco_13_by_director[\"Castellucci\"] = []\n",
    "global_3d_coco_13_by_director[\"Warlikowski\"] = []\n",
    "for global_3d_coco_13 in global_3d_coco_13_by_video[:10]:\n",
    "    global_3d_coco_13_by_director[\"BillTJones\"].extend(global_3d_coco_13)\n",
    "for global_3d_coco_13 in global_3d_coco_13_by_video[10:20]:\n",
    "    global_3d_coco_13_by_director[\"Castellucci\"].extend(global_3d_coco_13)\n",
    "for global_3d_coco_13 in global_3d_coco_13_by_video[20:30]:\n",
    "    global_3d_coco_13_by_director[\"Warlikowski\"].extend(global_3d_coco_13)\n",
    "global_3d_coco_13_by_director[\"BillTJones\"].extend(global_3d_coco_13_by_video[30])\n",
    "\n",
    "billtjones_mean_global_3d_coco_13 = np.mean(global_3d_coco_13_by_director[\"BillTJones\"], axis=0)\n",
    "castellucci_mean_global_3d_coco_13 = np.mean(global_3d_coco_13_by_director[\"Castellucci\"], axis=0)\n",
    "warlikowski_mean_global_3d_coco_13 = np.mean(global_3d_coco_13_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "billtjones_median_global_3d_coco_13 = np.median(global_3d_coco_13_by_director[\"BillTJones\"], axis=0)\n",
    "castellucci_median_global_3d_coco_13 = np.median(global_3d_coco_13_by_director[\"Castellucci\"], axis=0)\n",
    "warlikowski_median_global_3d_coco_13 = np.median(global_3d_coco_13_by_director[\"Warlikowski\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert video motion, distance summary statistics into feature vectors\n",
    "# Also extract per-video average embeddings for classification test\n",
    "# And write the feature summary stats to a TSV file\n",
    "\n",
    "video_labels = []\n",
    "video_features = []\n",
    "\n",
    "video_mean_poem_embeddings = []\n",
    "video_median_poem_embeddings = []\n",
    "video_mean_action_embeddings = []\n",
    "video_median_action_embeddings = []\n",
    "video_mean_global_3d_coco_13 = []\n",
    "video_median_global_3d_coco_13 = []\n",
    "\n",
    "summary_metrics_to_skip = [\"overlaps\", \"seconds\", \"director\", \"mean_3d_global\", \"mean_poem_embedding\", \"median_poem_embedding\", \"mean_action_embedding\", \"median_3d_global\", \"median_action_embedding\"]\n",
    "metrics_to_skip = [\"seconds\", \"poses_ppf\", \"tracks_per_shot\", \"interpose_dist\", \"director\", \"mean_3d_global\", \"mean_poem_embedding\", \"median_poem_embedding\", \"mean_action_embedding\", \"median_3d_global\", \"median_action_embedding\"]\n",
    "subkeys_to_skip = [\"count\", \"skewness\", \"kurtosis\"]\n",
    "slugs_to_skip = [\"overlaps_stdev\", \"overlaps_kurtosis\", \"overlaps_skewness\"]\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "summary_file = open(\"stylometry/video_summary_stats.tsv\", \"w\")\n",
    "summary_file.write(\"video\\tdirector\")\n",
    "\n",
    "for i, video_name in enumerate(all_video_stats):\n",
    "\n",
    "    if video_name.startswith(\"Don_Giovanni-\"):\n",
    "        continue\n",
    "\n",
    "    video_labels.append(all_video_stats[video_name][\"director\"])\n",
    "    video_mean_poem_embeddings.append(all_video_stats[video_name][\"mean_poem_embedding\"])\n",
    "    video_median_poem_embeddings.append(all_video_stats[video_name][\"median_poem_embedding\"])\n",
    "    video_mean_action_embeddings.append(all_video_stats[video_name][\"mean_action_embedding\"])\n",
    "    video_median_action_embeddings.append(all_video_stats[video_name][\"median_action_embedding\"])\n",
    "    video_mean_global_3d_coco_13.append(all_video_stats[video_name][\"mean_3d_global\"])\n",
    "    video_median_global_3d_coco_13.append(all_video_stats[video_name][\"median_3d_global\"])\n",
    "\n",
    "    # First build the feature vec for the numpy array\n",
    "    feature_vec = []\n",
    "    for metric in all_video_stats[video_name]:\n",
    "        if metric in metrics_to_skip:\n",
    "            continue\n",
    "        for subkey in all_video_stats[video_name][metric]:\n",
    "            if subkey in subkeys_to_skip:\n",
    "                continue\n",
    "            feature_slug = metric + \"_\" + subkey\n",
    "            if feature_slug in slugs_to_skip:\n",
    "                continue\n",
    "            if i == 0:\n",
    "                feature_names.append(feature_slug)\n",
    "\n",
    "            feature_vec.append(all_video_stats[video_name][metric][subkey])\n",
    "    video_features.append(feature_vec)\n",
    "\n",
    "    # Then build the list of summary stats for the TSV file\n",
    "    summary_vec = []\n",
    "    for metric in all_video_stats[video_name]:\n",
    "        if metric in summary_metrics_to_skip:\n",
    "            continue\n",
    "        for subkey in all_video_stats[video_name][metric]:\n",
    "            if subkey in subkeys_to_skip:\n",
    "                continue\n",
    "            feature_slug = metric + \"_\" + subkey\n",
    "            if feature_slug in slugs_to_skip:\n",
    "                continue\n",
    "            if i == 0:\n",
    "                summary_file.write(f\"\\t{feature_slug}\")\n",
    "\n",
    "            summary_vec.append(all_video_stats[video_name][metric][subkey])\n",
    "\n",
    "    if i == 0:\n",
    "        summary_file.write(\"\\n\") \n",
    "    summary_stats = [video_name, all_video_stats[video_name]['director']] + summary_vec\n",
    "    summary_file.write(\"\\t\".join([str(stat) for stat in summary_stats]) + \"\\n\")\n",
    "\n",
    "summary_file.close()\n",
    "\n",
    "video_feat_matrix = np.array(video_features)\n",
    "\n",
    "label_colormap = {0: \"red\", 1: \"green\", 2: \"blue\", 3: \"orange\"}\n",
    "label_colors = [label_colormap[l] for l in video_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Dumbest\" possible classifier, using \"leave one out\" approach based on video features\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, video_name in enumerate(video_names[:31]):\n",
    "\n",
    "    # XXX Note that this overwrites some values from the cells above\n",
    "    # (but just run those cells again to repopulate the values)\n",
    "    video_features_by_director = {}\n",
    "\n",
    "    video_features_by_director[\"BillTJones\"] = []\n",
    "    video_features_by_director[\"Castellucci\"] = []\n",
    "    video_features_by_director[\"Warlikowski\"] = []\n",
    "\n",
    "    for e, _ in enumerate(video_names[:31]):\n",
    "        if e == i:\n",
    "            continue\n",
    "        if e < 10 or e == 30:\n",
    "            video_features_by_director[\"BillTJones\"].append(video_features[e])\n",
    "        elif e >= 10 and e < 20:\n",
    "            video_features_by_director[\"Castellucci\"].append(video_features[e])\n",
    "        else:\n",
    "            video_features_by_director[\"Warlikowski\"].append(video_features[e])\n",
    "\n",
    "    billtjones_mean_video_features = np.mean(video_features_by_director[\"BillTJones\"], axis=0)\n",
    "    castellucci_mean_video_features = np.mean(video_features_by_director[\"Castellucci\"], axis=0)\n",
    "    warlikowski_mean_video_features = np.mean(video_features_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "    directors_matrix = np.stack((billtjones_mean_video_features, castellucci_mean_video_features, warlikowski_mean_video_features), axis=0)\n",
    "\n",
    "    test_video_features = video_features[i]\n",
    "    true_label = video_labels[i]\n",
    "\n",
    "    sims = cosine_similarity(directors_matrix, np.array([test_video_features]))\n",
    "\n",
    "    pred_label = np.argmax(sims)\n",
    "    print(video_name, \"Prediction:\", pred_label, \"True:\", true_label)\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Hits:\", hits, \"Misses:\", misses, f\"{hits*100/(hits+misses):.2f}%\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(video_labels[:31], predictions)\n",
    "#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"BillTJones\", \"Castellucci\", \"Warlikowski\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay.from_predictions(video_labels, predictions, display_labels=[\"BillTJones\", \"Castellucci\", \"Warlikowski\"], colorbar=False)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Dumbest\" possible classifier, using \"leave one out\" approach based on POEM embeddings\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, video_name in enumerate(video_names):\n",
    "\n",
    "    # XXX Note that this overwrites some values from the cells above\n",
    "    # (but just run those cells again to repopulate the values)\n",
    "    poem_embeddings_by_director = {}\n",
    "\n",
    "    poem_embeddings_by_director[\"BillTJones\"] = []\n",
    "    poem_embeddings_by_director[\"Castellucci\"] = []\n",
    "    poem_embeddings_by_director[\"Warlikowski\"] = []\n",
    "\n",
    "    for e, _ in enumerate(video_names[:31]):\n",
    "        if e == i:\n",
    "            continue\n",
    "        if e < 10 or e == 30:\n",
    "            poem_embeddings_by_director[\"BillTJones\"].extend(movelet_poem_embeds_by_video[e])\n",
    "        elif e >= 10 and e < 20:\n",
    "            poem_embeddings_by_director[\"Castellucci\"].extend(movelet_poem_embeds_by_video[e])\n",
    "        else:\n",
    "            poem_embeddings_by_director[\"Warlikowski\"].extend(movelet_poem_embeds_by_video[e])\n",
    "\n",
    "    billtjones_poem_embedding = np.mean(poem_embeddings_by_director[\"BillTJones\"], axis=0)\n",
    "    castellucci_poem_embedding = np.mean(poem_embeddings_by_director[\"Castellucci\"], axis=0)\n",
    "    warlikowski_poem_embedding = np.mean(poem_embeddings_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "    directors_matrix = np.stack((billtjones_poem_embedding, castellucci_poem_embedding, warlikowski_poem_embedding), axis=0)\n",
    "\n",
    "    avg_video_embedding = video_mean_poem_embeddings[i]\n",
    "    true_label = video_labels[i]\n",
    "\n",
    "    sims = cosine_similarity(directors_matrix, np.array([avg_video_embedding]))\n",
    "\n",
    "    pred_label = np.argmax(sims)\n",
    "    print(video_name, \"Prediction:\", pred_label, \"True:\", true_label)\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Hits:\", hits, \"Misses:\", misses, f\"{hits*100/(hits+misses):.2f}%\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(video_labels, predictions)\n",
    "#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"BillTJones\", \"Castellucci\", \"Warlikowski\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay.from_predictions(video_labels, predictions, display_labels=[\"BillTJones\", \"Castellucci\", \"Warlikowski\"], colorbar=False)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Dumbest\" possible classifier, using \"leave one out\" approach based on AVA embeddings\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, video_name in enumerate(video_names):\n",
    "\n",
    "    # XXX Note that this overwrites some values from the cells above\n",
    "    # (but just run those cells again to repopulate the values)\n",
    "    action_embeddings_by_director = {}\n",
    "\n",
    "    action_embeddings_by_director[\"BillTJones\"] = []\n",
    "    action_embeddings_by_director[\"Castellucci\"] = []\n",
    "    action_embeddings_by_director[\"Warlikowski\"] = []\n",
    "\n",
    "    for e, _ in enumerate(video_names):\n",
    "        if e == i:\n",
    "            continue\n",
    "        if e < 10 or e == 30:\n",
    "            action_embeddings_by_director[\"BillTJones\"].extend(action_embeds_by_video[e])\n",
    "        elif e >= 10 and e < 20:\n",
    "            action_embeddings_by_director[\"Castellucci\"].extend(action_embeds_by_video[e])\n",
    "        else:\n",
    "            action_embeddings_by_director[\"Warlikowski\"].extend(action_embeds_by_video[e])\n",
    "\n",
    "    billtjones_action_embedding = np.mean(action_embeddings_by_director[\"BillTJones\"], axis=0)\n",
    "    castellucci_action_embedding = np.mean(action_embeddings_by_director[\"Castellucci\"], axis=0)\n",
    "    warlikowski_action_embedding = np.mean(action_embeddings_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "    directors_matrix = np.stack((billtjones_action_embedding, castellucci_action_embedding, warlikowski_action_embedding), axis=0)\n",
    "\n",
    "    avg_video_embedding = video_mean_action_embeddings[i]\n",
    "    true_label = video_labels[i]\n",
    "\n",
    "    sims = cosine_similarity(directors_matrix, np.array([avg_video_embedding]))\n",
    "\n",
    "    pred_label = np.argmax(sims)\n",
    "    print(video_name, \"Prediction:\", pred_label, \"True:\", true_label)\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Hits:\", hits, \"Misses:\", misses, f\"{hits*100/(hits+misses):.2f}%\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(video_labels, predictions)\n",
    "#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"BillTJones\", \"Castellucci\", \"Warlikowski\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay.from_predictions(video_labels, predictions, display_labels=[\"BillTJones\", \"Castellucci\", \"Warlikowski\"], colorbar=False)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Dumbest\" possible classifier, using \"leave one out\" approach based on 3d global poses\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, video_name in enumerate(video_names):\n",
    "\n",
    "    # XXX Note that this overwrites some values from the cells above\n",
    "    # (but just run those cells again to repopulate the values)\n",
    "    global_3d_coco_13_by_director = {}\n",
    "\n",
    "    global_3d_coco_13_by_director[\"BillTJones\"] = []\n",
    "    global_3d_coco_13_by_director[\"Castellucci\"] = []\n",
    "    global_3d_coco_13_by_director[\"Warlikowski\"] = []\n",
    "\n",
    "    for e, _ in enumerate(video_names):\n",
    "        if e == i:\n",
    "            continue\n",
    "        if e < 10 or e == 30:\n",
    "            global_3d_coco_13_by_director[\"BillTJones\"].extend(global_3d_coco_13_by_video[e])\n",
    "        elif e >= 10 and e < 20:\n",
    "            global_3d_coco_13_by_director[\"Castellucci\"].extend(global_3d_coco_13_by_video[e])\n",
    "        else:\n",
    "            global_3d_coco_13_by_director[\"Warlikowski\"].extend(global_3d_coco_13_by_video[e])\n",
    "\n",
    "    billtjones_gobal_3d_coco_13 = np.mean(global_3d_coco_13_by_director[\"BillTJones\"], axis=0)\n",
    "    castellucci_global_3d_coco_13 = np.mean(global_3d_coco_13_by_director[\"Castellucci\"], axis=0)\n",
    "    warlikowski_global_3d_coco_13 = np.mean(global_3d_coco_13_by_director[\"Warlikowski\"], axis=0)\n",
    "\n",
    "    directors_matrix = np.stack((billtjones_gobal_3d_coco_13, castellucci_global_3d_coco_13, warlikowski_global_3d_coco_13), axis=0)\n",
    "\n",
    "    avg_video_global_3d_coco_13 = video_mean_global_3d_coco_13[i]\n",
    "    true_label = video_labels[i]\n",
    "\n",
    "    sims = cosine_similarity(directors_matrix, np.array([avg_video_global_3d_coco_13]))\n",
    "\n",
    "    pred_label = np.argmax(sims)\n",
    "    print(video_name, \"Prediction:\", pred_label, \"True:\", true_label)\n",
    "\n",
    "    predictions.append(pred_label)\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Hits:\", hits, \"Misses:\", misses, f\"{hits*100/(hits+misses):.2f}%\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(video_labels, predictions)\n",
    "#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"BillTJones\", \"Castellucci\", \"Warlikowski\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay.from_predictions(video_labels, predictions, display_labels=[\"BillTJones\", \"Castellucci\", \"Warlikowski\"], colorbar=False)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic 3D plot of pose coordinates\n",
    "\n",
    "COCO_13_SKELETON = [\n",
    "    (12, 10),\n",
    "    (10, 8),\n",
    "    (13, 11),\n",
    "    (11, 9),\n",
    "    (8, 9),\n",
    "    (2, 8),\n",
    "    (3, 9),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    "    (5, 7),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "]\n",
    "\n",
    "COCO_COLORS = [\n",
    "    \"orangered\",\n",
    "    \"orange\",\n",
    "    \"blue\",\n",
    "    \"lightblue\",\n",
    "    \"darkgreen\",\n",
    "    \"red\",\n",
    "    \"lightgreen\",\n",
    "    \"pink\",\n",
    "    \"plum\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"saddlebrown\",\n",
    "    \"mediumorchid\",\n",
    "    \"gray\",\n",
    "    \"salmon\",\n",
    "    \"chartreuse\",\n",
    "    \"lightgray\",\n",
    "    \"darkturquoise\",\n",
    "    \"goldenrod\",\n",
    "]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "#pose_3d = np.array(billtjones_gobal_3d_coco_13).reshape(13,3)\n",
    "#pose_3d = np.array(castellucci_global_3d_coco_13).reshape(13,3)\n",
    "pose_3d = np.array(warlikowski_global_3d_coco_13).reshape(13,3)\n",
    "\n",
    "\n",
    "ax.scatter(pose_3d[:,0], pose_3d[:,1], pose_3d[:,2])\n",
    "\n",
    "for i, seg in enumerate(COCO_13_SKELETON):\n",
    "    #line_color = ImageColor.getrgb(COCO_COLORS[i])\n",
    "    plt.plot([pose_3d[seg[0] - 1][0], pose_3d[seg[1] - 1][0]], [pose_3d[seg[0] - 1][1], pose_3d[seg[1] - 1][1]], [pose_3d[seg[0] - 1][2], pose_3d[seg[1] - 1][2]], 'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic visualization of the avg values of various movement/distance stats by video/director\n",
    "\n",
    "for i, ftn in enumerate(feature_names):\n",
    "    print(ftn)\n",
    "    #print(video_feat_matrix[i, :])\n",
    "    plt.scatter(video_labels, video_feat_matrix[:, i], c=label_colors)\n",
    "    plt.xticks([0, 1, 2], [\"BillTJones\", \"Castellucci\", \"Warlikowski\"], rotation=45)\n",
    "    ax = plt.gca()\n",
    "    for j, video_name in enumerate(all_video_stats):\n",
    "        ax.annotate(video_name, (video_labels[j], video_feat_matrix[j, i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier on all but one; see where that one is classified,\n",
    "# repeat for all videos (leave-one-out).\n",
    "# Potentially repeat the process for different random seeds, if the\n",
    "# classification algorithm is nondeterministic\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#import xgboost as xgb\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "N_SEEDS = 1\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "#X = video_feat_matrix\n",
    "X = video_mean_poem_embeddings\n",
    "#X = video_mean_action_embeddings\n",
    "#X = video_mean_global_3d_coco_13\n",
    "\n",
    "for r in range(N_SEEDS):\n",
    "    for h, held_out_vector in enumerate(X):\n",
    "\n",
    "        X_train = np.delete(X, h, 0)\n",
    "        y_train = np.delete(video_labels, h, 0)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=r)\n",
    "\n",
    "        #clf = MLPClassifier()\n",
    "        #clf = NuSVC(random_state=42)\n",
    "        clf = GaussianNB()\n",
    "        #clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2)\n",
    "        #clf = RandomForestClassifier(random_state=r)\n",
    "        clf.fit(X_train, y_train)\n",
    "        #clf.fit(X_train,  y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "        #pred = clf.predict(held_out_vector.reshape(1, -1))[0]\n",
    "        pred = clf.predict([held_out_vector])[0]\n",
    "\n",
    "        if pred == video_labels[h]:\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "        actual.append(video_labels[h])\n",
    "        predicted.append(pred)\n",
    "\n",
    "        #print(\"For \", video_names[h], \"pred is\", pred, \"true is\", video_labels[h])\n",
    "\n",
    "print(f\"Hits: {hits}, Misses: {misses}, {hits*100/(hits+misses):.2f}%\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "cm_display = metrics.ConfusionMatrixDisplay.from_predictions(actual, predicted, display_labels=[\"BillTJones\", \"Castellucci\", \"Warlikowski\"], colorbar=False)\n",
    "plt.title(\"GNB: AVA\")\n",
    "cm_display.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Another way to do it\n",
    "# scoring = ['precision_macro', 'recall_macro']\n",
    "# clf = RandomForestClassifier(random_state=0)\n",
    "# scores = cross_validate(clf, video_feat_matrix, video_labels, scoring=scoring)\n",
    "# print(\"precision scores:\", scores[\"test_precision_macro\"], \"recall scores:\", scores[\"test_recall_macro\"])\n",
    "\n",
    "N_SEEDS = 10\n",
    "\n",
    "print(\"10-fold cross validation with 10 random seeds for video feature vectors\")\n",
    "all_scores = []\n",
    "for i in range(N_SEEDS):\n",
    "    r = random.seed(datetime.now().timestamp())\n",
    "    clf = RandomForestClassifier(random_state=r)\n",
    "    #clf = GaussianNB()\n",
    "    cv = KFold(n_splits=10, random_state=r)\n",
    "    scores = cross_val_score(\n",
    "        clf, video_feat_matrix, video_labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    all_scores.extend(scores)\n",
    "print('Accuracy: %.3f ,\\nStandard Deviations :%.3f' %\n",
    "    (np.mean(all_scores), np.std(all_scores)))\n",
    "\n",
    "print(\"10-fold cross validation with 10 random seeds for POEM vectors\")\n",
    "for i in range(N_SEEDS):\n",
    "    r = random.seed(datetime.now().timestamp())\n",
    "    clf = RandomForestClassifier(random_state=r)\n",
    "    #clf = GaussianNB()\n",
    "    cv = KFold(n_splits=10, random_state=r)\n",
    "    scores = cross_val_score(\n",
    "        clf, video_mean_poem_embeddings, video_labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    all_scores.extend(scores)\n",
    "print('Accuracy: %.3f ,\\nStandard Deviations :%.3f' %\n",
    "    (np.mean(all_scores), np.std(all_scores)))\n",
    "\n",
    "print(\"10-fold cross validation with 10 random seeds for action vectors\")\n",
    "for i in range(N_SEEDS):\n",
    "    r = random.seed(datetime.now().timestamp())\n",
    "    clf = RandomForestClassifier(random_state=r)\n",
    "    #clf = GaussianNB()\n",
    "    cv = KFold(n_splits=10, random_state=r)\n",
    "    scores = cross_val_score(\n",
    "        clf, video_mean_action_embeddings, video_labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    all_scores.extend(scores)\n",
    "print('Accuracy: %.3f ,\\nStandard Deviations :%.3f' %\n",
    "    (np.mean(all_scores), np.std(all_scores)))\n",
    "\n",
    "print(\"10-fold cross validation with 10 random seeds for global 3D poses\")\n",
    "for i in range(N_SEEDS):\n",
    "    r = random.seed(datetime.now().timestamp())\n",
    "    clf = RandomForestClassifier(random_state=r)\n",
    "    #clf = GaussianNB()\n",
    "    cv = KFold(n_splits=10, random_state=r)\n",
    "    scores = cross_val_score(\n",
    "        clf, video_mean_global_3d_coco_13, video_labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    all_scores.extend(scores)\n",
    "print('Accuracy: %.3f ,\\nStandard Deviations :%.3f' %\n",
    "    (np.mean(all_scores), np.std(all_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(video_mean_poem_embeddings, video_labels, test_size=0.33, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(video_feat_matrix, video_labels, test_size=0.33, random_state=42)#\n",
    "X_train, X_test, y_train, y_test = train_test_split(video_mean_global_3d_coco_13, video_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "#clf = GaussianNB()\n",
    "#clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "#clf.fit(X_train,  y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "start_time = time.time()\n",
    "perm_importance = permutation_importance(\n",
    "    clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "vector_names = [f\"feature {i}\" for i in range(len(video_mean_poem_embeddings[0]))]\n",
    "\n",
    "coco_13_points = [\"nose\",  # 1\n",
    "    \"left_shoulder\",  # 2\n",
    "    \"right_shoulder\",  # 3\n",
    "    \"left_elbow\",  # 4\n",
    "    \"right_elbow\",  # 5\n",
    "    \"left_wrist\",  # 6\n",
    "    \"right_wrist\",  # 7\n",
    "    \"left_hip\",  # 8\n",
    "    \"right_hip\",  # 9\n",
    "    \"left_knee\",  # 10\n",
    "    \"right_knee\",  # 11\n",
    "    \"left_ankle\",  # 12\n",
    "    \"right_ankle\",  # 13\n",
    "]\n",
    "\n",
    "keypoint_names = [[f\"{item}X\", f\"{item}Y\", f\"{item}Z\"] for item in coco_13_points]\n",
    "keypoint_names = sum(keypoint_names, [])\n",
    "\n",
    "#clf_importances = pd.Series(perm_importance.importances_mean, index=vector_names)\n",
    "#clf_importances = pd.Series(perm_importance.importances_mean, index=feature_names)\n",
    "clf_importances = pd.Series(perm_importance.importances_mean, index=keypoint_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "clf_importances.plot.bar(yerr=perm_importance.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances (permutation): \")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "ax.tick_params(axis='x', labelrotation=40)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "#plt.barh(np.array(vector_names)[sorted_idx], perm_importance.importances_mean[sorted_idx], xerr=perm_importance.importances_std[sorted_idx])\n",
    "#plt.barh(np.array(feature_names)[sorted_idx], perm_importance.importances_mean[sorted_idx], xerr=perm_importance.importances_std[sorted_idx])\n",
    "plt.barh(np.array(keypoint_names)[sorted_idx], perm_importance.importances_mean[sorted_idx], xerr=perm_importance.importances_std[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar poses to a video's (or director's) average embedding\n",
    "# (can either search only that video, or all of them)\n",
    "\n",
    "pose_subquery = \"TRUE\"\n",
    "\n",
    "query = f\"\"\"\n",
    "            SELECT pose.video_id, video.video_name, pose.frame, pose.pose_idx, pose.bbox, pose.norm, pose.keypoints, poem_embedding <=> '{billtjones_mean_poem_embedding.tolist()}' AS distance, frame.shot AS shot FROM pose, frame, video\n",
    "            WHERE {pose_subquery} AND video.id = pose.video_id AND frame.video_id = pose.video_id AND pose.frame = frame.frame ORDER BY distance\n",
    "            LIMIT 500\n",
    "        \"\"\"\n",
    "\n",
    "similar_poses = await db._pool.fetch(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot/draw most similar poses to the average embedding\n",
    "\n",
    "import imageio.v3 as iio\n",
    "from lib.pose_drawing import pad_and_excerpt_image, draw_armatures\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "UPSCALE = 5\n",
    "\n",
    "async def get_frame_and_highlight_pose(video_id, frame, keypoints, bbox):\n",
    "    \n",
    "    keypoints = np.array([round(kp) for kp in keypoints]).reshape((13, 3))\n",
    "    xmin = int(np.rint(np.min(keypoints[:, 0])))\n",
    "    ymin = int(np.rint(np.min(keypoints[:, 1])))\n",
    "    xmax = int(np.rint(np.max(keypoints[:, 0])))\n",
    "    ymax = int(np.rint(np.max(keypoints[:, 1])))\n",
    "\n",
    "    video = await db.get_video_by_id(video_id)\n",
    "    video_path = f\"/videos/{video['video_name']}\"\n",
    "    frame_img = iio.imread(video_path, index=frame - 1, plugin=\"pyav\")\n",
    "\n",
    "    frame_img = Image.fromarray(np.uint8(frame_img)).convert('RGB')\n",
    "\n",
    "    frame_img = frame_img.resize((frame_img.width * UPSCALE, frame_img.height * UPSCALE))\n",
    "\n",
    "    overlay_img = ImageDraw.Draw(frame_img)\n",
    "\n",
    "    overlay_img = draw_armatures(keypoints, overlay_img, None)\n",
    "\n",
    "    overlay_img.rectangle([(xmin * UPSCALE, ymin * UPSCALE), (xmax * UPSCALE, ymax * UPSCALE)], outline =\"purple\", width = 4 * UPSCALE) \n",
    "\n",
    "    return frame_img\n",
    "\n",
    "\n",
    "async def get_cropped_pose_on_image(video_id, frame, keypoints):\n",
    "\n",
    "    keypoints = np.array([round(kp) for kp in keypoints]).reshape((13, 3))\n",
    "    xmin = int(np.rint(np.min(keypoints[:, 0])))\n",
    "    ymin = int(np.rint(np.min(keypoints[:, 1])))\n",
    "    xmax = int(np.rint(np.max(keypoints[:, 0])))\n",
    "    ymax = int(np.rint(np.max(keypoints[:, 1])))\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "\n",
    "    video = await db.get_video_by_id(video_id)\n",
    "    video_path = f\"/videos/{video['video_name']}\"\n",
    "    frame_img = iio.imread(video_path, index=frame - 1, plugin=\"pyav\")\n",
    "\n",
    "    excerpted_img = pad_and_excerpt_image(frame_img, xmin, ymin, width, height)\n",
    "\n",
    "    excerpted_img = Image.fromarray(np.uint8(excerpted_img)).convert('RGB')\n",
    "\n",
    "    excerpted_img = excerpted_img.resize((width * UPSCALE, height * UPSCALE))\n",
    "\n",
    "    overlay_img = ImageDraw.Draw(excerpted_img)\n",
    "\n",
    "    overlay_img = draw_armatures(keypoints, overlay_img, None, xmin, ymin)\n",
    "\n",
    "    return excerpted_img\n",
    "\n",
    "for pose in similar_poses[150:200]:\n",
    "\n",
    "    pose_image = await get_cropped_pose_on_image(pose.get(\"video_id\"), pose.get(\"frame\"), pose.get(\"keypoints\"))\n",
    "    frame_image = await get_frame_and_highlight_pose(pose.get(\"video_id\"), pose.get(\"frame\"), pose.get(\"keypoints\"), pose.get(\"bbox\"))\n",
    "\n",
    "    print(pose.get(\"video_name\"), \"frame\", pose.get(\"frame\"), \"pose\", pose.get(\"pose_idx\"))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(frame_image)\n",
    "    plt.show()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(pose_image)\n",
    "    plt.show()\n",
    "    #display(example_image)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar actions to a video's (or director's) average embedding\n",
    "# (can either search only that video, or all of them)\n",
    "\n",
    "pose_subquery = \"TRUE\"\n",
    "\n",
    "query = f\"\"\"\n",
    "            SELECT pose.video_id, video.video_name, pose.frame, pose.pose_idx, pose.bbox, pose.norm, pose.keypoints, action_labels, ava_action <=> '{billtjones_mean_action_embedding.tolist()}' AS distance, frame.shot AS shot FROM pose, frame, video\n",
    "            WHERE {pose_subquery} AND video.id = pose.video_id AND frame.video_id = pose.video_id AND pose.frame = frame.frame ORDER BY distance\n",
    "            LIMIT 500\n",
    "        \"\"\"\n",
    "\n",
    "similar_actions = await db._pool.fetch(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action_pose in similar_actions[:20]:\n",
    "    print(action_pose.get(\"video_name\"), \"frame\", action_pose.get(\"frame\"), \"pose\", action_pose.get(\"pose_idx\"))\n",
    "    print(action_pose.get(\"action_labels\"))\n",
    "\n",
    "    frame_image = await get_frame_and_highlight_pose(action_pose.get(\"video_id\"), action_pose.get(\"frame\"), action_pose.get(\"keypoints\"), action_pose.get(\"bbox\"))\n",
    "    pose_image = await get_cropped_pose_on_image(action_pose.get(\"video_id\"), action_pose.get(\"frame\"), action_pose.get(\"keypoints\"))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(frame_image)\n",
    "    plt.show()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(pose_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project large numbers of embeddings into 2D, attempt to colorize by director\n",
    "# and highlight where directors' (and specific videos'?) avg embeddings are.\n",
    "# NOTE: Running on the full embeddings set would exceed the RAM of most (maybe all)\n",
    "# machines, so we sample by SAMPLE_RATE.\n",
    "# Note also that there are almost no identical embeddings, and their dimensionality\n",
    "# is such that rounding the values and removing duplicates only decreases their size\n",
    "# by a tiny fraction.\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "SAMPLE_RATE = 100 # 1/100 poses will be used\n",
    "\n",
    "# For use with the Tensorflow Embedding Projector or similar visualizations\n",
    "poem_metadata = open(\"stylometry/sampled_pose_metadata.tsv\", \"w\")\n",
    "poem_metadata.write(\"Director\\tWork\\n\")\n",
    "\n",
    "all_poem_embeddings = []\n",
    "embedding_label_colors = []\n",
    "\n",
    "# XXX Gets messy due to the first 10 Bill T. Jones videos being indices 0-9, and the last one\n",
    "# being at the end of the list (index 30). Need to make sure metadata aligns with sampled embeddings\n",
    "\n",
    "e = 0\n",
    "for w in [*range(10), 30]:\n",
    "    dir_name = \"BillTJones\"\n",
    "    video_name = video_names[w]\n",
    "    work_embeds = movelet_poem_embeds_by_video[w]\n",
    "    for embed in work_embeds:\n",
    "        if e % SAMPLE_RATE == 0:\n",
    "            poem_metadata.write(f\"{dir_name}\\t{video_name}\\n\")\n",
    "        e += 1\n",
    "for w in range(10, 20):\n",
    "    dir_name = \"Castellucci\"\n",
    "    video_name = video_names[w]\n",
    "    work_embeds = movelet_poem_embeds_by_video[w]\n",
    "    for embed in work_embeds:\n",
    "        if e % SAMPLE_RATE == 0:\n",
    "            poem_metadata.write(f\"{dir_name}\\t{video_name}\\n\")\n",
    "        e += 1\n",
    "for w in range(20, 30):\n",
    "    dir_name = \"Warlikowski\"\n",
    "    video_name = video_names[w]\n",
    "    work_embeds = movelet_poem_embeds_by_video[w]\n",
    "    for embed in work_embeds:\n",
    "        if e % SAMPLE_RATE == 0:\n",
    "            poem_metadata.write(f\"{dir_name}\\t{video_name}\\n\")\n",
    "        e += 1\n",
    "\n",
    "# for w, work_embeds in enumerate(movelet_poem_embeds_by_video):\n",
    "#     video_name = video_names[w]\n",
    "#     if w < 10 or w == 30:\n",
    "#         dir_name = \"BillTJones\"\n",
    "#     elif w < 20:\n",
    "#         dir_name = \"Castellucci\"\n",
    "#     else:\n",
    "#         dir_name = \"Warlikowski\"\n",
    "#     for embed in work_embeds:\n",
    "#         if e % SAMPLE_RATE == 0:\n",
    "#             poem_metadata.write(f\"{dir_name}\\t{video_name}\\n\")\n",
    "#         e += 1\n",
    "\n",
    "i = 0\n",
    "for emb in poem_embeddings_by_director[\"BillTJones\"]:\n",
    "    if i % SAMPLE_RATE == 0:\n",
    "        all_poem_embeddings.append(emb)\n",
    "        embedding_label_colors.append(label_colormap[0])\n",
    "    i +=1\n",
    "for emb in poem_embeddings_by_director[\"Castellucci\"]:\n",
    "    if i % SAMPLE_RATE == 0:\n",
    "        all_poem_embeddings.append(emb)\n",
    "        embedding_label_colors.append(label_colormap[1])\n",
    "    i += 1\n",
    "for emb in poem_embeddings_by_director[\"Warlikowski\"]:\n",
    "    if i % SAMPLE_RATE == 0:\n",
    "        all_poem_embeddings.append(emb)\n",
    "        embedding_label_colors.append(label_colormap[2])\n",
    "    i += 1\n",
    "\n",
    "all_poem_embeddings.extend(video_mean_poem_embeddings)\n",
    "\n",
    "for i, video_name in enumerate(video_names):\n",
    "    if i < 10 or i == 30:\n",
    "        dir_name = \"BillTJones\"\n",
    "    elif i < 20:\n",
    "        dir_name = \"Castellucci\"\n",
    "    else:\n",
    "        dir_name = \"Warlikowski\"\n",
    "    poem_metadata.write(f\"{dir_name}\\t{video_name}_AVG\\n\")\n",
    "\n",
    "video_colors = [\"red\"] * 10\n",
    "video_colors.extend([\"green\"] * 10)\n",
    "video_colors.extend([\"blue\"] * 10)\n",
    "video_colors.extend([\"red\"])\n",
    "\n",
    "all_poem_embeddings.extend([billtjones_mean_poem_embedding, castellucci_mean_poem_embedding, warlikowski_mean_poem_embedding])\n",
    "#all_poem_embeddings.extend([billtjones_median_poem_embedding, castellucci_median_poem_embedding, warlikowski_median_poem_embedding])\n",
    "\n",
    "poem_metadata.write(\"BillTJones\\tDirector_Average\\n\")\n",
    "poem_metadata.write(\"Castellucci\\tDirector_Average\\n\")\n",
    "poem_metadata.write(\"Warlikowski\\tDirector_Average\\n\")\n",
    "\n",
    "print(\"CALCULATING UMAP PROJECTION ON\", len(all_poem_embeddings), \"POEM embeddings\")\n",
    "\n",
    "clusterable_embedding = umap.UMAP(\n",
    "    metric=\"cosine\",\n",
    "    n_neighbors=25,\n",
    "    min_dist=0,\n",
    "    n_components=2,\n",
    "    n_jobs=8,\n",
    ").fit_transform(all_poem_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# XXX This gets messy...\n",
    "# all but the last 34 embeddings are the individual sampled pose embeddings for each video,\n",
    "# followed by the average embedding for each video (n=31) and for each director (n=3)\n",
    "plt.scatter(clusterable_embedding[:-34, 0], clusterable_embedding[:-34, 1], s=2, alpha=0.3, c=embedding_label_colors)\n",
    "plt.scatter(clusterable_embedding[-34:-3, 0], clusterable_embedding[-34:-3, 1], s=50, alpha=0.8, marker=\"D\", c=video_colors, edgecolors=[\"black\"]* 31)\n",
    "plt.scatter(clusterable_embedding[-3:, 0], clusterable_embedding[-3:, 1], s=200, marker=\"H\", c=[\"red\", \"green\", \"blue\"], edgecolors=[\"black\", \"black\", \"black\"])\n",
    "\n",
    "# ax = plt.gca()\n",
    "# for s in range(30):\n",
    "#     ax.annotate(video_slugs[s], (clusterable_embedding[-33 + s, 0], clusterable_embedding[-33 + s, 1]))\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='BillTJones')\n",
    "green_patch = mpatches.Patch(color='green', label='Castellucci')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Warlikowski')\n",
    "plt.legend(handles=[red_patch, green_patch, blue_patch])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "poem_metadata.close()\n",
    "\n",
    "with open(\"stylometry/sampled_pose_embeddings.tsv\", \"w\") as poem_data:\n",
    "    for emb in all_poem_embeddings:\n",
    "        poem_data.write(\"\\t\".join([str(e) for e in emb]))\n",
    "        poem_data.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project large numbers of embeddings into 2D, attempt to colorize by director\n",
    "# and highlight where directors' (and specific videos'?) avg embeddings are.\n",
    "# NOTE: Running on the full embeddings set would exceed the RAM of most (maybe all)\n",
    "# machines, so we sample by SAMPLE_RATE.\n",
    "# Note also that there are almost no identical embeddings, and their dimensionality\n",
    "# is such that rounding the values and removing duplicates only decreases their size\n",
    "# by a tiny fraction.\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "SAMPLE_RATE = 100 # 1/100 poses will be used\n",
    "\n",
    "all_action_embeddings = []\n",
    "embedding_label_colors = []\n",
    "\n",
    "for i, emb in enumerate(action_embeddings_by_director[\"BillTJones\"]):\n",
    "    if i % SAMPLE_RATE == 0:\n",
    "        all_action_embeddings.append(emb)\n",
    "        embedding_label_colors.append(label_colormap[0])\n",
    "for i, emb in enumerate(action_embeddings_by_director[\"Castellucci\"]):\n",
    "    if i % SAMPLE_RATE == 0:\n",
    "        all_action_embeddings.append(emb)\n",
    "        embedding_label_colors.append(label_colormap[1])\n",
    "for i, emb in enumerate(action_embeddings_by_director[\"Warlikowski\"]):\n",
    "    if i % SAMPLE_RATE == 0:\n",
    "        all_action_embeddings.append(emb)\n",
    "        embedding_label_colors.append(label_colormap[2])\n",
    "\n",
    "all_action_embeddings.extend([billtjones_mean_action_embedding, castellucci_mean_action_embedding, warlikowski_mean_action_embedding])\n",
    "#all_action_embeddings.extend([billtjones_median_action_embedding, castellucci_median_action_embedding, warlikowski_median_action_embedding])\n",
    "\n",
    "print(\"CALCULATING UMAP PROJECTION ON\", len(all_action_embeddings), \"ACTION embeddings\")\n",
    "\n",
    "clusterable_embedding = umap.UMAP(\n",
    "    metric=\"cosine\",\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.0,\n",
    "    n_components=2,\n",
    "    n_jobs=8,\n",
    ").fit_transform(all_action_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(clusterable_embedding[:-3, 0], clusterable_embedding[:-3, 1], s=2, alpha=0.3, c=embedding_label_colors)\n",
    "#plt.scatter(clusterable_embedding[-6:-3, 0], clusterable_embedding[-6:-3, 1], s=200, marker=\"s\", c=[\"red\", \"green\", \"blue\"], edgecolors=[\"white\", \"white\", \"white\"])\n",
    "plt.scatter(clusterable_embedding[-3:, 0], clusterable_embedding[-3:, 1], s=200, marker=\"h\", c=[\"red\", \"green\", \"blue\"], edgecolors=[\"white\", \"white\", \"white\"])\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='BillTJones')\n",
    "green_patch = mpatches.Patch(color='green', label='Castellucci')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Warlikowski')\n",
    "\n",
    "plt.legend(handles=[red_patch, green_patch, blue_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Z distances seem to be exaggerated -- apparently by 200:1? See PHALP phalp/utils/utils.py\n",
    "- Focal length is 5000 (pixels?) -- not sure how to use this\n",
    "\n",
    "Based on body part ratios (i.e., considering average of all shoulder widths, waist widths, upper arm lengths and lower arm lengths) the projected 3D pose coordinate units appear to be in meters.\n",
    "\n",
    "## Metrics of interest for director-to-video clustering analysis\n",
    "\n",
    "- Difference of action recognition embedding vectors. How best to compare? Just cosine distance of averaged embeddings? Some kind of multi-dimensional KL divergence? Would be nice to be able to take into account the normalized histogram of values in each of the 62 elements of the action recognition vectors.\n",
    "\n",
    "- Difference of \"held\" 3D pose vectors/representations. How to compare? Maybe just 3D Euclidean distance between averaged normalized poses? As flattened embeddings, they'd be 100 x 100 x 100 = 1,000,000-element vectors (curse of dimensionality) unless of course they're downsampled further to like 10 x 10\n",
    "\n",
    "## Motion:\n",
    "\n",
    "- Normalized: per-pixel movement within a pose's frame of reference, for each movelet -- already calculated\n",
    "- Sidereal (relative to the \"scenery\"): normalized per-meter (?) movement for each movelet -- need to join movelets with poses (again)\n",
    "\n",
    "## Closeness of figures in frames/shots:\n",
    "\n",
    "One primary shortcoming of this is that we usually can't see the whole stage, and reconstructing the entire scene based on multiple camera views (if those are provided at all) is beyond current technology, though various NeRF/Gaussian Splatting methods are getting closer. So we're mostly just stuck with what we get in each shot, which makes this analysis highly \"contaminated\" by the cinematography/cinematic style. Only way to sidestep this for now is to note that we're including multiple films for each stage director, so hopefully the influence of the filmic cinematography of any given recording will be attenuated/smoothed over when they've compared in \"bulk.\"\n",
    "\n",
    "- For each populated frame in the film, with the denominator = # of frames with actual people detected in them. Ignore unpopulated frames; incorporate the frame rate to get all of these metrics per second:\n",
    "  - Get the dist of number of people per populated frame/sec\n",
    "- For the rest of these, only consider frames with multiple people:\n",
    "  - Get the distance dist between people per populated frame/sec (see below for calculations)\n",
    "  - Get the number of overlapping people per populated frame/sec (see below for calculations)\n",
    "\n",
    "### Secondary analyses\n",
    "\n",
    "- Maybe try to count and distribution-ize the number of distinct people in a shot -- this is essentially the number of tracks in a shot (tracking is done by appearance, including costumes though without much weight given to faces). Could help to cover the case of people coming into and out of a shot. Though it may just further confuse the issue, and in any case this should be very sensitive to shot length -- so may need to normalize by shot length somehow for the distribution to be useful.\n",
    "\n",
    "### Alternative shot-based analysis (probably not necessary)\n",
    "\n",
    "- For each shot:\n",
    "\n",
    "  - For the first (?) frame in the shot:\n",
    "\n",
    "    - For each pose in the frame:\n",
    "\n",
    "      - Get its position as the median of x, y, z/200 (?) coords\n",
    "      - Also get its 3D bounding box if doing collision detection\n",
    "\n",
    "    - Compute Euclidean distance matrix of all of the pose centroids, put this into a distribution, extract statistics\n",
    "      - Determine how many (if any) 3D bounding boxes intersect (touching) []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
