{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import imageio.v3 as iio\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from matplotlib import colormaps, colors\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import umap\n",
    "\n",
    "from mime_db import MimeDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = \"\" # Just the name of the video file, no path\n",
    "\n",
    "# DeepFace and ArcFace resize/crop face images to 152x152 and 112x112 pixels,\n",
    "# respectively, so an image upsized from smaller than 100xH px is not useful.\n",
    "WIDTH_THRESHOLD = 100  # pixels of width\n",
    "\n",
    "FACE_FEATURES = 4096 # 4096 for DeepFace, 512 for ArcFace\n",
    "\n",
    "video_path = Path(\"videos\", VIDEO_FILE)\n",
    "\n",
    "# Connect to the database\n",
    "db = await MimeDb.create()\n",
    "\n",
    "# Get video metadata\n",
    "video_name = video_path.name\n",
    "video_id = await db.get_video_id(video_name)\n",
    "video_id = video_id[0][\"id\"]\n",
    "\n",
    "#video_poses = await db.get_pose_data_from_video(video_id)\n",
    "video_poses = await db.get_poses_with_faces(video_id)\n",
    "\n",
    "poses_df = pd.DataFrame.from_records(video_poses, columns=video_poses[0].keys())\n",
    "\n",
    "# This averages the feature vectors of every frame of a given face/track.\n",
    "# The resulting vector is of limited use, because a pose's face\n",
    "# orientation can change a lot across a movement track, and pretty much\n",
    "# all face feature extractors are sensitive to face orientation (pose).\n",
    "# Also assumes that every track always follows the same person throughout\n",
    "# its duration, which is correct in theory (but in practice ...)\n",
    "def average_face_embeddings(embeddings):\n",
    "    # Not sure why all these conversions are necessary...\n",
    "    embeddings = np.array([embed for embed in embeddings])\n",
    "    # Need at least 5 frames/poses to consider this a reliable face\n",
    "    if embeddings.shape[0] < 5:\n",
    "        avg_embed = None\n",
    "    else:\n",
    "        avg_embed = np.mean(embeddings, axis=0)\n",
    "    return [avg_embed] * embeddings.shape[0]\n",
    "\n",
    "poses_df[\"face_width\"] = poses_df.apply(lambda p: 0 if p[\"face_bbox\"] is None else p[\"face_bbox\"][2], axis=1)\n",
    "\n",
    "# similarity_threshold = dst.findThreshold(\"DeepFace\", \"cosine\")\n",
    "# logging.info(\n",
    "#     f\"Suggested similarity threshold for DeepFace+cosine is {similarity_threshold}\"\n",
    "# )\n",
    "\n",
    "print(\"Total poses:\", len(poses_df))\n",
    "\n",
    "poses_df = poses_df[\n",
    "    (\n",
    "        (~np.isnan(poses_df[\"face_confidence\"]))\n",
    "        & (poses_df[\"face_confidence\"] > 0)\n",
    "        & (poses_df[\"face_width\"] > WIDTH_THRESHOLD)\n",
    "    )\n",
    "].reset_index()\n",
    "\n",
    "print(\"Poses with usable faces:\", len(poses_df))\n",
    "\n",
    "# We can use the largest face image as a thumbnail/rep (if desired)\n",
    "poses_df[\"face_area\"] = poses_df.apply(lambda p: 0 if p[\"face_bbox\"] is None else p[\"face_bbox\"][2] * p[\"face_bbox\"][3], axis=1)\n",
    "\n",
    "poses_df[\"face_avg\"] = poses_df.groupby([\"track_id\"])[\"face_embedding\"].transform(\n",
    "    average_face_embeddings\n",
    ")\n",
    "\n",
    "poses_df[\"face_embedding\"] = poses_df[\"face_embedding\"].apply(lambda p: p[:FACE_FEATURES])\n",
    "\n",
    "rep_poses_df = poses_df.iloc[\n",
    "    (\n",
    "        # poses_df.groupby([\"track_id\"])[\"face_area\"].idxmax()\n",
    "        # This is always pretty close to 1...\n",
    "        poses_df.groupby([\"track_id\"])[\"face_confidence\"].idxmax()\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Faces representing a track:\", len(rep_poses_df))\n",
    "\n",
    "print(\n",
    "    rep_poses_df[[\"track_id\", \"frame\", \"face_bbox\", \"face_area\", \"face_confidence\"]].head(5)\n",
    ")\n",
    "\n",
    "X = rep_poses_df[\"face_embedding\"].to_list()\n",
    "im_labels = rep_poses_df[[\"track_id\", \"frame\"]].applymap(str).agg('_'.join, axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_embedding = umap.UMAP(\n",
    "    random_state=42,\n",
    ").fit_transform(X)\n",
    "\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterable_embedding = umap.UMAP(\n",
    "    n_neighbors=10,\n",
    "    min_dist=0.0,\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    ").fit_transform(X)\n",
    "\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting clustering model\")\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=3, min_samples=4) # , max_cluster_size=15\n",
    "hdb.fit(X)\n",
    "labels = hdb.labels_.tolist()\n",
    "\n",
    "assigned_faces = 0\n",
    "\n",
    "for cluster_id in range(-1, max(labels) + 1):\n",
    "    print(\"Faces in cluster\", cluster_id, labels.count(cluster_id))\n",
    "    if cluster_id != -1:\n",
    "        assigned_faces += labels.count(cluster_id)\n",
    "\n",
    "print(\"assigned\", assigned_faces, \"track faces out of\", len(labels), round(assigned_faces/len(labels),4))\n",
    "    \n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fitting UMAP preclustered model\")\n",
    "save_images = True\n",
    "plot_images = True\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=3, min_samples=4) # , max_cluster_size=15\n",
    "hdb.fit(clusterable_embedding)\n",
    "labels = hdb.labels_.tolist()\n",
    "\n",
    "assigned_faces = 0\n",
    "\n",
    "for cluster_id in range(-1, max(labels) + 1):\n",
    "    if save_images and not os.path.isdir(str(cluster_id)):\n",
    "        os.mkdir(str(cluster_id))\n",
    "    print(\"Faces in cluster\", cluster_id, labels.count(cluster_id))\n",
    "    if cluster_id != -1:\n",
    "        assigned_faces += labels.count(cluster_id)\n",
    "\n",
    "print(\"assigned\", assigned_faces, \"track faces out of\", len(labels), round(assigned_faces/len(labels),4))\n",
    "\n",
    "if save_images:\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        try:\n",
    "            cluster_pose = rep_poses_df.iloc[i]\n",
    "        except Exception as e:\n",
    "            print(\"Error referencing face\", i)\n",
    "            print(e)\n",
    "            continue\n",
    "        x, y, w, h = [round(coord) for coord in cluster_pose[\"face_bbox\"]]\n",
    "        video_handle = f\"/videos/{video_name}\"\n",
    "        img = iio.imread(video_handle, index=cluster_pose[\"frame\"] - 1, plugin=\"pyav\")\n",
    "        img_region = img[y : y + h, x : x + w]\n",
    "        iio.imwrite(f\"{cluster_id}/{i}.jpg\", img_region, extension=\".jpeg\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.gca()\n",
    "\n",
    "if plot_images:\n",
    "    cm = colormaps[\"Spectral\"]\n",
    "    norm = colors.Normalize(vmin=-1, vmax=max(labels))\n",
    "    \n",
    "    ax.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1])\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        try:\n",
    "            cluster_pose = rep_poses_df.iloc[i]\n",
    "        except Exception as e:\n",
    "            print(\"Error referencing face\", i)\n",
    "            print(e)\n",
    "            continue\n",
    "        x, y, w, h = [round(coord) for coord in cluster_pose[\"face_bbox\"]]\n",
    "        video_handle = f\"/videos/{video_name}\"\n",
    "        img = iio.imread(video_handle, index=cluster_pose[\"frame\"] - 1, plugin=\"pyav\")        \n",
    "        img_region = img[y : y + h, x : x + w]\n",
    "        img = Image.fromarray(img_region)\n",
    "        img.thumbnail((20, 20), resample=Image.Resampling.LANCZOS)\n",
    "        ab = AnnotationBbox(OffsetImage(np.asarray(img)), (clusterable_embedding[i, 0], clusterable_embedding[i, 1]))\n",
    "        ab.patch.set_linewidth(1)\n",
    "        ab.patch.set(color=cm(norm(cluster_id)))\n",
    "\n",
    "        ax.add_artist(ab)\n",
    "else:\n",
    "    ax.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=labels, cmap='Spectral', s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
